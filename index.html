<!DOCTYPE html>
<html lang="en">

<!-- Color: https://htmlcolorcodes.com/zh/yanse-ming/ -->

<head>
	<link rel="shortcut icon" href="files/ucla.jpg" />
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111713571-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());
		gtag('config', 'UA-111713571-1');
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css"
		integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">

	<!-- Icon CSS-->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css"
		integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

	<!-- Custom styles for this template -->
	<link rel="stylesheet" href="files/panlu.css">
</head>


<style>
	h3 {
		font-size: 30px;
	}
</style>


<title>Pan Lu</title>


<body style="font-size: 18px">
	<!-- Navigation Labels -->
	<!-- <nav class="navbar navbar-expand-md navbar-dark fixed-top" style="background-color: #003262;"> -->
	<nav class="navbar navbar-expand-md navbar-dark fixed-top" style="background-color: #003262; font-size: 22px">

		<a class="navbar-brand" href="#" style="font-size: 22px">Pan Lu</a>

		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
			<span class="navbar-toggler-icon"></span>
		</button>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Publications">Publications</a>
				</li>
				<!-- <li class="nav-item">
					<a class="nav-link" href="#Education">Education</a>
				</li> -->
				<li class="nav-item">
					<a class="nav-link" href="#Experience">Experience</a>
				</li>
				<!-- <li class="nav-item">
					<a class="nav-link" href="#Honors">Honors</a>
				</li> -->
				<li class="nav-item">
					<a class="nav-link" href="#Teaching">Teaching</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Service">Service</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Tweets">Tweets</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Contact">Contact</a>
				</li>
			</ul>
		</div>
	</nav>

	<!-- Who am I -->
	<div class="container" style="padding-top: 30px; font-size: 18px">
		<div class="row">
			<div class="col-md-3" , style="padding-right: 40px">
				<br>
				<img class="img-responsive img-rounded" src="files/panlu.jpg" alt=""
					style="max-width: 240px; border:1px solid black"><br>
			</div>
			<div class="col-md-9">
				<br>
				<p> I am a fourth-year PhD candidate in the Computer Science Department, University of California, Los
					Angeles.

					I am a member of the <a href="https://vcla.stat.ucla.edu/" target="_blank">Center for Vision,
						Cognition, Learning and Autonomy</a>, supervised by Professor
					<a href="http://www.stat.ucla.edu/~sczhu/" target="_blank">Song-Chun Zhu</a>.

					I am closely working with
					Dr. <a href="http://ashwinkalyan.com/" target="_blank">Ashwin Kalyan</a>,
					Professor <a href="https://www.cs.columbia.edu/~zhouyu/" target="_blank">Zhou Yu</a>,
					Professor <a href="http://web.cs.ucla.edu/~kwchang/" target="_blank">Kai-Wei Chang</a>, and
					Professor <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a> at UCLA.

					Before that, I received my Master's degree in Computer Science at Tsinghua University, supervised by
					Professor <a href="http://dbgroup.cs.tsinghua.edu.cn/wangjy/" target="_blank">Jianyong Wang</a>.
				</p>

				<p>
					My long-term research goal is to build machines that can <strong>reason</strong> like humans and
					<strong>collaborate</strong> with humans. I am interested in NLP and machine reasoning, including
					the following topics:
				<ul>
					<li>
						<strong>Mathematical reasoning</strong> in educational scenarios (e.g., math, geometry, science)
					</li>
					<li>
						<strong>Trustworthy NLP models</strong> that are explainable and human-in-loop
					</li>
					<li>
						<strong>Conversational agents</strong> that are value-aware and socially intelligent
					</li>
					<li>
						<strong>Multimodal learning</strong> in vision-and-language applications (e.g., VQA, image
						captioning)
					</li>
				</ul>
				</p>
			</div>
		</div>

		<!-- Icons -->
		<div style="font-size: 24px;">
			<br>
			<!-- icon size: large fa-2x, small, fa-lg -->
			<a href="mailto:lupantech@gmail.com">
				<font color="gray"><i class="fas fa-envelope fa-lg"></i></font>
			</a>&emsp;
			<a target="_blank" href="https://scholar.google.com/citations?user=IyucsdQAAAAJ&hl=en">
				<font color="gray"><i class="ai ai-google-scholar ai-lg"></i></font>
			</a>&emsp;
			<a target="_blank" href="https://github.com/lupantech">
				<font color="gray"><i class="fab fa-github fa-lg"></i></font>
			</a>&emsp;
			<a target="_blank" href="https://www.linkedin.com/in/pan-lu-9308909a/">
				<font color="gray"><i class="fab fa-linkedin fa-lg"></i></font>
			</a>&emsp;
			<!-- <a target="_blank" href="https://www.facebook.com/xxx"><font color="black"><i class="fab fa-facebook-square fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;&nbsp; -->
			<!-- <a target="_blank" href="files/CV_Pan Lu.pdf"><font color="black"><i class="ai ai-cv ai-lg"></i></font></a> -->
			<a target="_blank" href="https://twitter.com/lupantech">
				<font color="gray"><i class="fab fa-twitter fa-lg"></i></font>
			</a>&emsp;
			<a href="https://twitter.com/lupantech?ref_src=twsrc%5Etfw" class="twitter-follow-button"
				data-show-count="false">Follow @lupantech</a>
			<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
		</div>
	</div><br><br>

	<!-- News -->
	<div class="container">
		<h3 id="News" style="">News</h3>
		<hr>
		<ul>
			<li>
				<strong>[09/2022]&nbsp; <font color="red">New!</font></strong>
				One paper on dynamic prompt learning for math reasoning is submitted to <b>Preprint</b>.
			</li>
			<li>
				<strong>[09/2022]&nbsp; <font color="red">New!</font></strong>
				One paper on chain-of-thought reasoning for
				<b><a href="https://scienceqa.github.io/" target="_blank">ScienceQA</a></b>
				is accepted to <b>NeurIPS 2022</b>.
			</li>
			<li>
				<strong>[09/2022]&nbsp; <font color="red">New!</font></strong>
				One paper on chain-of-thought reasoning for
				<b><a href="https://scienceqa.github.io/" target="_blank">ScienceQA</a></b>
				is accepted to <b>NeurIPS 2022</b>.
			</li>
			<li>
				<strong>[07/2022]&nbsp; <font color="red">New!</font></strong>
				I am co-organizing the 2nd <b><a href="https://mathai2022.github.io/" target="_blank">MATH-AI</a></b>
				Workshop at <b>NeurIPS 2022</b>. See you in New Orleans!
			</li>
			<li>
				<strong>[07/2022]&nbsp; <font color="red">New!</font></strong>
				One paper on socially intelligent agents is accepted to <b>SIGDIAL 2022</b>.
			</li>
			<strong>[03/2022]&nbsp; <font color="red">New!</font></strong>
			I am excited to join <b><a href="https://allenai.org/" target="_blank">Allen Institute for AI (AI2)</a></b>
			as a research intern!
			</li>
			<li>
				<strong>[03/2022]&nbsp; <font color="red">New!</font></strong>
				One paper on character animation sampling is submitted to <b>Preprint</b>.
			</li>
			<li>
				<strong>[02/2022]&nbsp; <font color="red">New!</font></strong>
				Data and code for NeurIPS 2021
				<b><a href="https://iconqa.github.io/" target="_blank">IconQA</a></b>
				are released now!
			</li>
			<li>
				<strong>[12/2021]&nbsp; <font color="red">New!</font></strong>
				Two papers are accepted to <b>AAAI 2022</b></font></b>.
			</li>
			<li>
				<strong>[10/2021]&nbsp; <font color="red">New!</font></strong>
				One paper on visual question answering for icon images (<b><a href="https://iconqa.github.io/"
						target="_blank">IconQA</a></b>) is accepted to <b>NeurIPS 2021</b></font></b>.
			</li>
			<li>
				<strong>[07/2021]&nbsp;</strong>
				I am co-organizing the <b><a href="https://mathai4ed.github.io/" target="_blank">MATHAI4ED</a></b>
				Workshop at <b>NeurIPS 2021</b>. Welcome to participate!
			</li>
			<li>
				<strong>[07/2021]&nbsp; </strong>
				Our workshop proposal for Math AI for Education (MATHAI4ED) is accepted to <b>NeurIPS 2021</b></font>
				</b>.
			</li>
			<li>
				<strong>[05/2021]&nbsp; </strong>
				One paper on interpretable geometry problem solving is accepted to <b>ACL 2021</b> as an <b>
					<font color="red">Oral Presentation</font>
				</b>.
			</li>
			<li>
				<strong>[05/2021]&nbsp; </font>
				</strong> One paper on social relation inference in dialogues is accepted to <b>ACL 2021</b> as an <b>
					<font color="red">Oral Presentation</font>
				</b>.
			</li>
			<li>
				<strong>[03/2021]</strong> One paper on socially intelligent agents is submitted to <b>Preprint</b>.
			</li>
			<li>
				<strong>[11/2019]</strong> One paper on personalized image caption is accepted to <b>AAAI 2020</b>.
			</li>
			<li>
				<strong>[05/2019]</strong> One paper on knowledge aware image-text matching is accepted to <b>IJCAI
					2019</b> as an <b>
					<font color="red">Oral Presentation</font>
				</b>.
			</li>
			<li>
				<strong>[03/2019]</strong> One paper on dynamic fusion for visual question answering is accepted to
				<b>CVPR 2019</b> as an <b>
					<font color="red">Oral Presentation</font>
				</b>.
			</li>
		</ul>
	</div><br>


	<!-- Publications -->
	<div class="container">
		<h3 id="Publications" style="">Selected Publications</h3>
		<hr>
		<!-- <font color="black">(* indicate equal contribution)</font><br><hr> -->

		<!-- TabMWP -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv22_dynamic2.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical
						Reasoning
					</font>
				</b><br>
				<b>Pan Lu</b>, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark,
				Ashwin Kalyan<br>
				<b><a href="https://nips.cc/" target="_blank">arXiv:2209.14610&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2209.14610" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/arxiv22_dynamic.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://https://tabmwp.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a> <small>[Data]&nbsp;</small></a>
				<a> <small>[Code]&nbsp;</small></a>
				<a href="bibs/arxiv22_dynamic.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- ScienceQA -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/neurips22_scienceqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question
						Answering</font>
				</b><br>
				<b>Pan Lu</b>, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter
				Clark,
				Ashwin Kalyan<br>
				<b><a href="https://nips.cc/" target="_blank">NeurIPS 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2209.09513" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/neurips22_scienceqa.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://scienceqa.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a> <small>[Data]&nbsp;</small></a>
				<a> <small>[Code]&nbsp;</small></a>
				<a href="bibs/neurips22_scienceqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- Mind -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv21_mind.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Towards Socially Intelligent Agents with Mental State Transition and Human
						Utility</font>
				</b><br>
				<a>Liang Qiu<sup>*</sup>, Yizhou Zhao<sup>*</sup>, Yuan Liang, <b>Pan Lu</b>, Weiyan Shi, Zhou Yu,
					Song-Chun Zhu</a><br>
				<b><a href="https://2022.sigdial.org/" target="_blank">SIGDIAL 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2103.07011" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/sigdial22_mind.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="bibs/sigdial22_mind.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- Animation -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv22_animation.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Triangular Character Animation Sampling with Motion, Emotion, and Relation
					</font>
				</b><br>
				<a>Yizhou Zhao, Liang Qiu, Wensi Ai, <b>Pan Lu</b>, Song-Chun Zhu</a><br>
				<b><a href="https://arxiv.org/abs/2203.04930" target="_blank">arXiv:2203.04930&nbsp;</a></b>
				<a href="https://arxiv.org/pdf/2203.04930.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/arxiv22_animation.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- AAAI22 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaai22_tangram.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Learning from the Tangram to Solve Mini Visual Tasks</font>
				</b><br>
				<a>Yizhou Zhao, Liang Qiu, <b>Pan Lu</b>, Feng Shi, Tian Han, Song-Chun Zhu</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-22/" target="_blank">AAAI 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2112.06113" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2112.06113.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<!-- <a href="" target="_blank"> <small>[Project]&nbsp;</small></a> -->
				<a href="https://github.com/yizhouzhao/Tangram" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/aaai22_tangram.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- AAAI22 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaai22_valuenet.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">ValueNet: A New Dataset for Human Value Driven Dialogue System</font>
				</b><br>
				<a>Liang Qiu, Yizhou Zhao, Jinchao Li, <b>Pan Lu</b>, Baolin Peng, Jianfeng Gao, Song-Chun Zhu</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-22/" target="_blank">AAAI 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2112.06346" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/aaai22_valuenet.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://liang-qiu.github.io/ValueNet/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<!-- <a href="" target="_blank"> <small>[Code]&nbsp;</small></a> -->
				<a href="bibs/aaai22_valuenet.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<!-- <font color="firebrick"><b>Oral Presentation</b></font> -->
			</div>
		</div>
		<hr>

		<!-- GenMotion -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv21_genmotion.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">GenMotion: Data-driven Motion Generators for Real-time Animation Synthesis
					</font>
				</b><br>
				<a>Yizhou Zhao, Wensi Ai, Liang Qiu, <b>Pan Lu</b>, Feng Shi, Tian Han, Song-Chun Zhu</a><br>
				<b><a href="https://arxiv.org/abs/2112.06060" target="_blank">arXiv:2112.06060&nbsp;</a></b>
				<a href="https://arxiv.org/pdf/2112.06060.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/arxiv21_genmotion.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- NeurIPS21 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/neurips21_iconqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language
						Reasoning</font>
				</b><br>
				<a><b>Pan Lu</b>, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang,
					Song-Chun Zhu</a><br>
				<b><a href="https://nips.cc/Conferences/2021" target="_blank">NeurIPS 2021&nbsp;</a></b>
				<a href="papers/neurips21_iconqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/neurips21_iconqa.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://iconqa.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/lupantech/IconQA" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/neurips21_iconqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Datasets and Benchmarks Track</b></font>
			</div>
		</div>
		<hr>

		<!-- ACL21 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/acl21_gps.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and
						Symbolic Reasoning</font>
				</b><br>
				<a><b>Pan Lu</b><sup>*</sup>, Ran Gong<sup>*</sup>, Shibiao Jiang<sup>*</sup>, Liang Qiu, Siyuan Huang,
					Xiaodan Liang, Song-Chun Zhu</a><br>
				<b><a href="https://2021.aclweb.org/" target="_blank">ACL 2021&nbsp;</a></b>
				<a href="https://aclanthology.org/2021.acl-long.528/" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/acl21_intergps.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://lupantech.github.io/inter-gps/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/lupantech/InterGPS" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/acl21_gps.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- ACL21 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/acl21_soc.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues
					</font>
				</b><br>
				<a>Liang Qiu, Yuan Liang, Yizhou Zhao, <b>Pan Lu</b>, Baolin Peng, Zhou Yu, Ying Nian Wu, Song-Chun
					Zhu</a><br>
				<b><a href="https://2021.aclweb.org/" target="_blank">ACL 2021&nbsp;</a></b>
				<a href="papers/acl21_soc.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/acl21_soc.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- AAAI20 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaai20_caption.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Learning Long- and Short-Term User Literal-Preference with Multimodal
						Hierarchical Transformer Network for Personalized Image Caption</font>
				</b><br>
				<a>Wei Zhang, Yue Ying, <strong>Pan Lu</strong>, Hongyuan Zha</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-20/" target="_blank">AAAI 2020&nbsp;</a></b>
				<a href="papers/aaai20_caption.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/aaai20_caption.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- IJCAI19 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/ijcai19_matching.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Knowledge Aware Semantic Concept Expansion for Image-Text Matching</font>
				</b><br>
				<a>Botian Shi, Lei Ji, <strong>Pan Lu</strong>, Nan Duan</a><br>
				<b><a href="https://ijcai19.org/" target="_blank">IJCAI 2019&nbsp;</a></b>
				<a href="papers/ijcai19_matching.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/ijcai19_matching.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- CVPR19 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/cvpr19_dynamicvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Dynamic Fusion with Intra- and Inter-modality Attention Flow for Visual Question
						Answering</font>
				</b><br>
				<a>Peng Gao, Zhengkai Jiang, Haoxuan You, <strong>Pan Lu</strong>, Steven CH Hoi, Xiaogang Wang,
					Hongsheng Li</a><br>
				<b><a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019&nbsp;</a></b>
				<a href="papers/cvpr19_dynamicvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://github.com/bupt-cist/DFAF-for-VQA.pytorch" target="_blank">
					<small>[Code]&nbsp;</small></a>
				<a href="bibs/cvpr19_dynamicvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- PAKDD19 -->
		<!-- <div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/pakdd2019_hybrid.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b><font color="black">A Novel Hybrid Sequential Model for Review-based Rating Prediction</font></b><br>
				<a>Yuanquan Lu, Wei Zhang, <strong>Pan Lu</strong>, Jianyong Wang</a><br>
				<b><a href="https://pakdd2019.medmeeting.org/en" target="_blank">PAKDD 2019&nbsp;</a></b>
				<a href="papers/pakdd2019_hybrid.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/pakdd2019_hybrid.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div><hr> -->

		<!-- ICDE19  -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/icde19_knowledge.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Knowledge-Aware Deep Dual Networks for Text-Based Mortality Prediction</font>
				</b><br>
				<a>Ning Liu, <strong>Pan Lu</strong>, Wei Zhang, Jianyong Wang</a><br>
				<b><a href="http://conferences.cis.umac.mo/icde2019/" target="_blank">ICDE 2019&nbsp;</a></b>
				<a href="papers/icde19_knowledge.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/icde19_knowledge.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- ECCV18 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/eccv18_hybridvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Question-Guided Hybrid Convolution for Visual Question Answering</font>
				</b><br>
				<a>Peng Gao, Hongsheng Li, Shuang Li, <strong>Pan Lu</strong>, Yikang Li, Steven Hoi, Xiaogang
					Wang</a><br>
				<b><a href="https://eccv2018.org/" target="_blank">ECCV 2018&nbsp;</a></b>
				<a href="papers/eccv18_hybridvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/eccv18_hybridvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- SIGKDD18 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/kdd18_rvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual
						Question Answering</font>
				</b><br>
				<a><strong>Pan Lu</strong>, Lei Ji, Wei Zhang, Nan Duan, Ming Zhou, Jianyong Wang</a><br>
				<b><a href="https://www.kdd.org/kdd2018/" target="_blank">SIGKDD 2018&nbsp;</a></b>
				<a href="papers/kdd18_rvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://github.com/lupantech/rvqa" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://www.youtube.com/watch?v=OQoqpRuY4L4" target="_blank"> <small>[Video]&nbsp;</small></a>
				<a href="bibs/kdd18_rvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- AAAI18 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaa18_dualvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Co-attending Free-form Regions and Detections with Multi-modal Multiplicative
						Feature Embedding for Visual Question Answering</font>
				</b><br>
				<a><strong>Pan Lu</strong>, Hongsheng Li, Wei Zhang, Jianyong Wang, Xiaogang Wang</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-18/" target="_blank">AAAI 2018&nbsp;</a></b>
				<a href="papers/aaa18_dualvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://github.com/lupantech/dual-mfa-vqa" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/aaa18_dualvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

	</div><br>


	<!-- Publications -->
	<div class="container">
		<h3 id="Publications" style="">Workshop Publications</h3>
		<hr>

		<!-- ACL21 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/neuripsws21_gps.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Theorem-Aware Geometry Problem Solving with Symbolic Reasoning and Theorem
						Prediction</font>
				</b><br>
				<a><b>Pan Lu</b><sup>*</sup>, Ran Gong<sup>*</sup>, Shibiao Jiang<sup>*</sup>, Liang Qiu, Siyuan Huang,
					Xiaodan Liang, Song-Chun Zhu</a><br>
				<b><a href="https://mathai4ed.github.io/" target="_blank">NeurIPS 2021 Workshop
						(MATHAI4ED)&nbsp;</a></b>
				<a href="https://mathai4ed.github.io/papers/papers/paper_8.pdf" target="_blank">
					<small>[Paper]&nbsp;</small></a>
				<a href="https://mathai4ed.github.io/papers/papers/paper_8.pdf" target="_blank">
					<small>[PDF]&nbsp;</small></a>
				<a href="https://lupantech.github.io/inter-gps/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/lupantech/InterGPS" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/neuripsws21_gps.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- NeurIPS21 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/neuripsws21_iconqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Towards Diagram Understanding and Cognitive Reasoning in Icon Question Answering
					</font>
				</b><br>
				<a><b>Pan Lu</b>, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang,
					Song-Chun Zhu</a><br>
				<b><a href="https://mathai4ed.github.io/" target="_blank">NeurIPS 2021 Workshop
						(MATHAI4ED)&nbsp;</a></b>
				<a href="https://mathai4ed.github.io/papers/papers/paper_12.pdf" target="_blank">
					<small>[Paper]&nbsp;</small></a>
				<a href="https://mathai4ed.github.io/papers/papers/paper_12.pdf" target="_blank">
					<small>[PDF]&nbsp;</small></a>
				<a href="https://iconqa.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/lupantech/IconQA" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/neuripsws21_iconqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

	</div><br>


	<!-- Education -->
	<div class="container">
		<h3 id="Education" style="">Education</h3>
		<hr>
		<ul>
			<li>
				<strong>University of California, Los Angeles</strong>, 2019.9 - present<br>
				Ph.D. Student in Computer Science
			</li>
			<li>
				<strong>Tsinghua University</strong>, 2015.8 - 2018.7<br>
				M.Eng. in Computer Science
			</li>
			<li>
				<strong>Beijing Institute of Technology</strong>, 2011.8 - 2015.7<br>
				B.S. in Electrical Engineering
			</li>
		</ul>
	</div><br>


	<!-- Experience -->
	<div class="container">
		<h3 id="Experience" style="">Experience</h3>
		<hr>
		<ul>
			<li>
				<strong>Research Intern</strong>, <em>Allen Institute for AI (AI2)</em>, 2022.3 - present <br>
				Advisors: Dr. <a href="http://ashwinkalyan.com/" target="_blank">Ashwin Kalyan</a>,
				Dr. <a href="https://allenai.org/team/peterc" target="_blank">Peter Clark</a>
			</li>
			<li>
				<strong>Graduate Student Researcher</strong>, <em>Center for Vision, Cognition, Learning, and
					Autonomy</em>, <em>UCLA</em>, 2019.9 - present<br>
				Advisor: Prof. <a href="http://www.stat.ucla.edu/~sczhu/" target="_blank">Song-Chun Zhu</a>
			</li>
			<li>
				<strong>Research Intern</strong>, <em>Document Intelligence Group</em>, <em>Adobe Research</em>, 2021.6
				- 2021.9 <br>
				Advisors: Dr. <a href="https://research.adobe.com/person/jiuxiang-gu/" target="_blank">Jiuxiang Gu</a>,
				Dr. <a href="https://research.adobe.com/person/tong-sun/" target="_blank">Tong Sun</a>
			</li>
			<li>
				<strong>Researcher Intern</strong>, <em>Basemodel Group</em>, <em>Face Plus</em>, 2019.3 - 2019.7 <br>
				Advisor: Dr. <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en"
					target="_blank">Xiangyu Zhang</a>
			</li>
			<li>
				<strong>Research Intern</strong>, <em>Natural Language Computing Group</em>, <em> Microsoft Research
					Asia</em>, 2017.11 - 2018.5<br>
				Advisors: Dr. <a href="https://www.microsoft.com/en-us/research/people/leiji/" target="_blank">Lei
					Ji</a>, Dr. <a href="https://www.microsoft.com/en-us/research/people/nanduan/" target="_blank">Nan
					Duan</a>, Dr. <a href="https://www.microsoft.com/en-us/research/people/mingzhou/"
					target="_blank">Ming Zhou</a>
			</li>
			<li>
				<strong>Research Assistant</strong>, <em>Multimedia Lab</em>, <em>Chinese University of Hong Kong</em>,
				2016.8 - 2017.10 <br>
				Advisors: Prof. <a href="https://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Hongsheng Li</a>, Prof. <a
					href="https://www.ee.cuhk.edu.hk/~xgwang/" target="_blank">Xiaogang Wang</a>
			</li>
			<li>
				<strong>Research Assistant</strong>, <em>Data Mining Group</em>, <em>Tsinghua University</em>, 2014.11 -
				2018.7 <br>
				Advisor: Prof. <a href="http://dbgroup.cs.tsinghua.edu.cn/wangjy/" target="_blank">Jianyong Wang</a>
			</li>
			<li>
				<strong>Research Intern</strong>, <em>Institute of Automation</em>, <em>Chinese Academy of
					Sciences</em>, 2014.7 - 2014.9 <br>
				Advisor: Prof. <a href="http://www.nlpr.ia.ac.cn/users/szli/" target="_blank">Stan Z. Li</a>
			</li>
		</ul>
	</div><br>


	<!-- Teaching -->
	<div class="container">
		<h3 id="Teaching" style="">Teaching</h3>
		<hr>
		<ul>
			<li>
				<strong>Teaching Assistant</strong>, <em>CS 249: Data Science Fundamentals</em><br>
				Graduate, UCLA, Fall 2022
			</li>
			<li>
				<strong>Guest Lecturer</strong>, <em>ECE C247: Neural Networks and Deep Learning</em><br>
				Graduate, UCLA, Winter 2022
			</li>
			<li>
				<strong>Teaching Assistant</strong>, <em>ECE C247: Neural Networks and Deep Learning</em><br>
				Graduate, UCLA, Winter 2022
			</li>
			<li>
				<strong>Teaching Assistant</strong>, <em>CS 111: Operating System Principles</a></em><br>
				Undergraduate, UCLA, Fall 2021
			</li>
			<li>
				<strong>Guest Lecturer</strong>, <em>Data Mining: Theory and Algorithms</em><br>
				Graduate, Tsinghua University, Fall 2017
			</li>
			<li>
				<strong>Teaching Assistant</strong>, <em>C Programming Language</a></em><br>
				Undergraduate, Tsinghua University, Spring 2016
			</li>
		</ul>
	</div><br>


	<!-- Service -->
	<div class="container">
		<h3 id="Service" style="">Professional Service</h3>
		<hr>
		<h4 id="Service" style="">Organizer</h3>
			<ul>
				<li>
					<strong>Primary organizer</strong>, NeurIPS 2022 Workshop on
					<a href="https://mathai2022.github.io/" target="_blank">MATH-AI: Toward Human-Level Mathematical
						Reasoning</a>,
					New Orleans, 2022.12
				</li>
				<li>
					<strong>Primary organizer</strong>, NeurIPS 2021 Workshop on
					<a href="https://mathai4ed.github.io/" target="_blank">Math AI for Education (MATHAI4ED): Bridging
						the Gap Between Research and Smart Education</a>,
					Virtual, 2021.12
				</li>
				<li> <strong>Chair</strong>,
					IEEE Student Branch at Tsinghua University, Beijing, 2015.10 - 2016.10
				</li>
			</ul>
			<h4 id="Service" style="">Program Committee Member</h3>
				<ul>
					<li> <strong>2023</strong>: AAAI</li>
					<li> <strong>2022</strong>: COLING, NeurIPS, CVPR, AAAI, ICLR</li>
					<li> <strong>2021</strong>: NeurIPS, ICCV, ICLR, CVPR</li>
					<li> <strong>2020</strong>: NeurIPS </li>
					<li> <strong>2019 and before</strong>: AAAI, SIGKDD, ICDM, PAKDD</li>
				</ul>
				<h4 id="Service" style="">Journal Reviewer</h3>
					<ul>
						<li>
							<a href="http://www.ieee-jas.org/" target="_blank">IEEE/CAA JAS</a>,
							<a href="http://www.aas.net.cn/CN/volumn/current.shtml" target="_blank">AAS</a>
						</li>
					</ul>
	</div><br>

	<!-- Awards -->
	<div class="container">
		<h3 id="Honors" style="">Selected Honors</h3>
		<hr>
		<ul>
			<li><strong>NeurIPS Travel Award</strong>, 2019</li>
			<li><strong>ICCV Travel Award</strong>, 2019</li>
			<li><strong>Outstanding Master Thesis Award</strong>, Tsinghua University, 2018</li>
			<li><strong>SIGKDD Travel Award</strong>, 2018</li>
			<li><strong>"Stars of Tomorrow" Excellent Intern Award</strong>, Microsoft Research, 2018</li>
			<li><strong>AAAI Travel Award</strong>, 2018, 2020</li>
			<li><strong>GuangHua Scholarship Award</strong>, Tsinghua University, 2016</li>
			<li><strong>Outstanding Undergraduate, Beijing</strong>, 2015</li>
			<li><strong>Champion of National College Student Innovation Conference</strong>, 2014</li>
			<li><strong>National Scholarship</strong> (<em>for top 2% students</em>), 2014</li>
			<li><strong>First Prize of America Mathematical Contest In Modeling (MCM)</strong>, 2014</li>
			<li><strong>President Teli Xuâ€™s Special Fellowship</strong> (<em>the highest honor for top 2
					undergraduates</em>), 2013</li>
		</ul>
	</div><br>

	<!-- Tweets -->
	<div class="container">
		<h3 id="Tweets" style="">Tweets</h3>
		<hr>
		<ul>
			<div class="container" style="text-align: left;">
				<!-- https://publish.twitter.com/?buttonType=FollowButton&query=%40lupantech&widget=Timeline -->
				<a class="twitter-timeline" data-width="50%" data-height="500" data-theme="light"
					href="https://twitter.com/lupantech?ref_src=twsrc%5Etfw">Tweets by lupantech</a>
				<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
			</div>
		</ul>
	</div><br>

	<!-- Contact -->
	<div class="container">
		<h3 id="Contact" style="">Contact</h3>
		<hr>
		Center for Vision, Cognition, Learning and Autonomy (VCLA) <br>
		Bolter Hall <br>
		580 Portola Plaza <br>
		Los Angeles, CA 90024 <br>
		lupantech [at] gmail [dot] com <br>
		<!-- <a href="mailto:lupantech@gmail.com">lupantech@gmail.com</a><br> -->
		<a href="https://scholar.google.com/citations?user=IyucsdQAAAAJ&hl=en" target="_blank">[<span
				style="color:#4285F4">G</span><span style="color:#DB4437">o</span><span
				style="color:#F4B400">o</span><span style="color:#4285F4">g</span><span
				style="color:#0F9D58">l</span><span style="color:#DB4437">e</span> Scholar]</a> &nbsp;| &nbsp;<a
			href="https://github.com/lupantech" target="_blank">[GitHub]</a> &nbsp;|&nbsp;
		<a href="https://www.linkedin.com/in/pan-lu-9308909a/" target="_blank">[LinkedIn]</a>
	</div>


	<!-- Footer -->
	<div class="container">
		<hr>
		<!-- <center>
			<footer>
				<p>&copy; Pan Lu 2019</p>
			</footer>
		</center> -->
		<div class="row">
			<div class="col-md-9">
				<p align="left">&copy; Pan Lu 2022</p>
			</div>
			<div class="col-md-3">
				<script type="text/javascript" id="clustrmaps"
					src="//cdn.clustrmaps.com/map_v2.js?d=kM7hb210psDEOVXSuHck6GmcfISVxTpZ0I6EmgBHZMQ&cl=ffffff&w=a"></script>
			</div>
		</div>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
		integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
		crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"
		integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4"
		crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js"
		integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1"
		crossorigin="anonymous"></script>

</body>

</html>