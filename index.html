<!DOCTYPE html>
<html lang="en">

<head>
	<link rel="shortcut icon" href="files/stanford.png" />
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111713571-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());
		gtag('config', 'UA-111713571-1');
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css"
		integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">

	<!-- Icon CSS-->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css"
		integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

	<!-- Custom styles for this template -->
	<link rel="stylesheet" href="files/panlu.css">
</head>


<title>Pan Lu</title>


<body style="">
	<!-- Navigation Labels -->
	<!-- <nav class="navbar navbar-expand-md navbar-dark fixed-top" style="background-color: #003262;"> -->
	<nav class="navbar navbar-expand-md navbar-dark fixed-top" style="background-color: #003262; font-size: 22px">

		<a class="navbar-brand" href="#" style="font-size: 22px">Pan Lu</a>

		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
			<span class="navbar-toggler-icon"></span>
		</button>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#News">News</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Publications">Publications</a>
				</li>
				<!-- <li class="nav-item">
					<a class="nav-link" href="#Experience">Experience</a>
				</li> -->
				<li class="nav-item">
					<a class="nav-link" href="#Teaching">Teaching</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Service">Service</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Awards">Awards</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Contact">Contact</a>
				</li>
			</ul>
		</div>
	</nav>

	<div class="container">
		<div class="col-md-3" id="div-image">
			<br>
			<img class="img-responsive img-rounded" src="files/pan.jpeg" alt=""
						style="max-width: 240px; border:1px solid black"><br>
			<!-- Icons -->
			<div style="font-size: 24px;">
				<br>
				<!-- icon size: large fa-2x, small, fa-lg -->
				<a href="mailto:lupantech@gmail.com">
					<font color="gray"><i class="fas fa-envelope fa-lg"></i></font>
				</a>&thinsp;
				<a target="_blank" href="https://scholar.google.com/citations?user=IyucsdQAAAAJ&hl=en">
					<font color="gray"><i class="ai ai-google-scholar ai-lg"></i></font>
				</a>&thinsp;
				<a target="_blank" href="https://www.semanticscholar.org/author/Pan-Lu/2887562">
					<font color="gray"><i class="ai ai-semantic-scholar ai-lg"></i></font>
				</a>&thinsp;
				<a target="_blank" href="https://github.com/lupantech">
					<font color="gray"><i class="fab fa-github fa-lg"></i></font>
				</a>&thinsp;
				<a target="_blank" href="https://www.linkedin.com/in/pan-lu-9308909a/">
					<font color="gray"><i class="fab fa-linkedin fa-lg"></i></font>
				</a>&thinsp;
				<!-- <a target="_blank" href="files/CV_Pan Lu.pdf"><font color="black"><i class="ai ai-cv ai-lg"></i></font></a> -->
				<a target="_blank" href="https://twitter.com/lupantech">
					<font color="gray"><i class="fab fa-twitter fa-lg"></i></font>
				</a>
				<br><br>
				<div class="container" style="text-align: left; padding-left: 30px">
					<a href="https://twitter.com/lupantech?ref_src=twsrc%5Etfw" class="twitter-follow-button"
						data-show-count="false" data-size="large" >Follow @lupantech</a>
					<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
				</div>
			</div>
		</div>
		<div class="container" id="div-bio">
			<div class="row">
				<div class="col-md-12">
					<br>
					<p> 
						I am a Postdoctoral Scholar at Stanford University. 
						I am affiliated with <a href="https://ai.stanford.edu/" target="_blank">Stanford AI Lab</a>, 
						<a href="https://www.james-zou.com/people" target="_blank">Zou's Group</a>, and <a href="https://yejinc.github.io/" target="_blank">Choi's xlab</a>, 
						where I am fortunate to be advised by Professor <a href="https://www.james-zou.com/" target="_blank">James Zou</a> and Professor <a href="https://yejinc.github.io/" target="_blank">Yejin Choi</a>.
					</p>
					<p>
						I received my Ph.D. in computer science from UCLA, where I was advised by  <a href="http://web.cs.ucla.edu/~kwchang/" target="_blank">Kai-Wei Chang</a> and <a href="http://www.stat.ucla.edu/~sczhu/" target="_blank">Song-Chun Zhu</a>. I was a member of <a href="https://web.cs.ucla.edu/~kwchang/members/" target="_blank">UCLA Natural Language Processing Group (UCLA NLP)</a>. Previously, I completed my M.S. in computer science at Tsinghua University, supervised by <a href="http://dbgroup.cs.tsinghua.edu.cn/wangjy/" target="_blank">Jianyong Wang</a>. 
						My research has been recognized with <a href="https://resources.paperdigest.org/2025/03/most-influential-iclr-papers-2025-03-version/" target="_blank">Most Influential ICLR Paper Award</a> (top-15 cited at ICLR 2024), <a href="https://resources.paperdigest.org/2024/09/most-influential-nips-papers-2024-09/" target="_blank">Most Influential NIPS Paper Award</a> (top-15 cited at NeurIPS 2022), <a href="https://knowledge-nlp.github.io/naacl2025/index.html" target="_blank">KnowledgeNLP 2025 Workshop Best Paper Award</a>, and EMNLP 2024 Best Paper Nomination — achievements made possible thanks to the support of my advisors and collaborators.
						I have been fortunate to receive recognition from 
						<a href="https://www.sciencehub.ucla.edu/2023-amazon-fellows/" target="_blank">Amazon PhD Fellowship</a>, 
						<a href="https://www.bloomberg.com/company/stories/introducing-the-sixth-cohort-of-bloomberg-data-science-ph-d-fellows-2023-2024/" target="_blank">Bloomberg Data Science Ph.D. Fellowship</a> (Global 9), 
						<a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-north-america" target="_blank">Qualcomm Innovation Fellowship</a> (18 winners), 
						<a href="https://grad.ucla.edu/funding/financial-aid/funding-for-continuing-students/dissertation-year-fellowship/" target="_blank">UCLA Dissertation Year Fellowship</a>, 
						and NeurIPS Scholar Award.
						<!-- and <a href="https://vcla.stat.ucla.edu/" target="_blank">the Center for Vision, Cognition, Learning, and Autonomy (VCLA)</a> -->
					</p>
					<p>
						My research goal is to develop intelligent machines that can <strong>reason</strong> and <strong>collaborate</strong> with humans for the common good. My primary focus lies in <strong>machine learning</strong> and <strong>natural language processing</strong>, particularly in <strong>machine reasoning</strong>, <strong>mathematical reasoning</strong>, and <strong>scientific discovery</strong>. My recent research interests include:
					</p>
					<ul>
						<li>
							<strong>Tool-Augmented LLMs</strong> and <strong>Agentic Systems</strong> for complex reasoning
							<a href="https://agentflow.stanford.edu/" target="_blank">[AgentFlow]</a>
							<a href="https://octotools.github.io/" target="_blank">[OctoTools]</a>
							<a href="https://arxiv.org/abs/2510.06217" target="_blank">[TaTToo]</a>
							<a href="https://arxiv.org/abs/2509.25370" target="_blank">[AgentDebug]</a>
							<a href="https://arxiv.org/abs/2504.04785" target="_blank">[W4S]</a>
							<!-- <a href="https://arxiv.org/abs/2501.06590" target="_blank">[ChemAgent]</a> -->
						</li>
						<li>
							<strong>Post-Training</strong> and <strong>Test-Time Training</strong> techniques for foundation models
							<a href="https://arxiv.org/abs/2405.19716" target="_blank">[STIC]</a> 
							<a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank">[LLaMA-Adapter]</a> 
							<a href="https://arxiv.org/abs/2303.16199" target="_blank">[LLaMA-Adapter V2]</a>
							<a href="https://arxiv.org/abs/2402.05935" target="_blank">[SPHINX-X]</a> 
							<a href="https://textgrad.com/" target="_blank">[TextGrad]</a>
							<a href="https://promptpg.github.io/" target="_blank">[PromptPG]</a>
						</li>
						<li>
							<strong>AI for Math</strong>: advancing mathematical reasoning capabilities of AI systems and LLMs across multimodal, knowledge-intensive, and real-world contexts
							<a href="https://arxiv.org/abs/2506.07927" target="_blank">[IneqMath]</a>
							<a href="https://mathvista.github.io/" target="_blank">[MathVista]</a>
							<a href="https://arxiv.org/abs/2403.14624" target="_blank">[MathVerse]</a>
							<a href="https://promptpg.github.io/" target="_blank">[PromptPG]</a>
							<a href="https://lupantech.github.io/inter-gps/" target="_blank">[Inter-GPS]</a>
							<a href="https://arxiv.org/abs/2110.13214" target="_blank">[IconQA]</a>
							<a href="https://arxiv.org/abs/2305.12524v1" target="_blank">[TheoremQA]</a>
							<a href="https://arxiv.org/abs/2212.10535" target="_blank">[DL4Math]</a>
							<a href="https://mathai2024.github.io/" target="_blank">[MATH-AI]</a>
						</li>
						<li>
							<strong>AI for Science</strong>: AI systems that facilitate scientific reasoning and scientific discovery
							<a href="https://scienceqa.github.io/" target="_blank">[ScienceQA]</a> 
							<a href="https://arxiv.org/abs/2307.10635" target="_blank">[SciBench]</a> 
							<!-- <a href="https://chameleon-llm.github.io/" target="_blank">[Chameleon]</a> -->
							<a href="https://www.arxiv.org/abs/2502.17504" target="_blank">[Protein-LLM]</a> 
							<a href="https://arxiv.org/abs/2501.06590" target="_blank">[ChemAgent]</a>
						</li>
					</ul>
					[25.09]
					<font color="red">
						 We are seeking students and collaborators to work on research in
						<b> agentic AI, post-training LLMs, reinforcement learning, AI for Math, AI for Science, and related fields</b>. 
					</font>
						A background in these fields is preferred but not strictly required. If you're interested in joining us, please apply via <b><a href="https://forms.gle/kP7eja1vckNPnGy88" target="_blank">this form</a></b>. For a faster response, kindly send me an email after submitting the form.
					<br>
				</div>
			</div>
		</div>
	</div>
	<br>

	<div class="container">
		<div class="container" id="div-twitter">
			<a class="twitter-timeline" id="twitter" data-width=50% data-height=650 data-theme=light
				href="https://twitter.com/lupantech?ref_src=twsrc%5Etfw">Tweets by lupantech</a>
			<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
		</div>
		<div class="container" id="div-news" >
			<h3 id="News" style="">News</h3>
			<hr>
			<ul>
				<li>
					<strong>[10/2025]&nbsp; <font color="red">New!</font></strong>
					Our trainable agentic framework <b><a href="https://agentflow.stanford.edu/" target="_blank">AgentFlow</a></b> for planning and tool use is available at
					<b><a href="https://arxiv.org/abs/2510.05592" target="_blank">Preprint</a></b>.
				</li>
				<li>
					<strong>[09/2025]&nbsp; <font color="red">New!</font></strong>
					Honored to be selected as a <b><a href="https://topscinet.com/scientist_profile/Lu,%20Pan/2017/?stype=single_year" target="_blank">World's Top 2% Scientists</a></b> by Elsevier!
				</li>
				<li>
					<strong>[09/2025]&nbsp; <font color="red">New!</font></strong>
					Excited to have our <b><a href="https://ineqmath.github.io/" target="_blank">IneqMath</a></b>
					paper accepted at <b><a href="https://nips.cc/Conferences/2025" target="_blank">NeurIPS 2025</a></b> as a <b>Spotlight</b> 🎉!
				</li>
				<li>
					<strong>[08/2025]&nbsp; <font color="red">New!</font></strong>
					Excited to win the <b><a href="https://www.renaissancephilanthropy.org/ai-for-math-fund-projects" target="_blank">AI for Math Fund Grant Award</a></b> ($1M Grant for two years) 🏆!
				</li>
				<li>
					<strong>[07/2025]&nbsp; <font color="red">New!</font></strong>
					Glad to co-organize the 5th <b><a href="https://mathai2025.github.io/" target="_blank">MATH-AI</a></b>
					Workshop at <b><a href="https://nips.cc/Conferences/2025" target="_blank">NeurIPS 2025</a></b>!
				</li>
				<li>
					<strong>[06/2025]&nbsp; <font color="red">New!</font></strong>
					Glad to co-organize the <b><a href="https://cvpr2025mutimodalmathreasoning.github.io/MM_math_reasoning/" target="_blank">Multimodal Mathematical Reasoning</a></b>
					Workshop at <b><a href="https://cvpr.thecvf.com/" target="_blank">CVPR 2025</a></b>!
				</li>
				<li>
					<strong>[06/2025]&nbsp; <font color="red">New!</font></strong>
					Our new study on solving inequality proofs with LLMs is available at
					<b><a href="https://arxiv.org/abs/2506.07927" target="_blank">Preprint</a></b>.
				</li>
				<li>
					<strong>[05/2025]&nbsp; <font color="red">New!</font></strong>
					Excited that
					<b><a href="https://octotools.github.io/" target="_blank">MathVista</a></b>
					has been recognized as the 
					<b><a href="https://resources.paperdigest.org/2025/03/most-influential-iclr-papers-2025-03-version/" target="_blank">🏆 Most Influential ICLR Paper</a></b> (ICLR-24)!
				</li>
				<li>
					<strong>[05/2025]&nbsp; <font color="red">New!</font></strong>
					Excited that <b><a href="https://octotools.github.io/" target="_blank">OctoTools</a></b> received the <b>🏆 Best Paper Award</b> at the <b><a href="https://knowledge-nlp.github.io/naacl2025/index.html" target="_blank">KnowledgeNLP Workshop</a></b>!
				</li>
				<li>
					<strong>[04/2025]&nbsp; <font color="red">New!</font></strong>
					Excited to be invited to serve as a Senior Area Chair for <b><a href="https://2025.emnlp.org/" target="_blank">EMNLP 2025</a></b>!
				</li>
				<li>
					<strong>[04/2025]&nbsp; <font color="red">New!</font></strong>
					Excited to be invited to serve as an Area Chair for <b><a href="https://nips.cc/Conferences/2025" target="_blank">NeurIPS 2025</a></b>!
				</li>

				<li>
					<strong>[03/2025]&nbsp; <font color="red">New!</font></strong>
					Thrilled to announce that <b><a href="https://textgrad.com/" target="_blank">TextGrad</a></b> is published in <b><a href="https://www.nature.com/articles/s41586-025-08661-4" target="_blank">Nature</a></b>!
				</li>
				<li>
					<strong>[02/2025]&nbsp; <font color="red">New!</font></strong>
					Our <b><a href="https://octotools.github.io/" target="_blank">OctoTools</a></b> agentic framework with extensible tools is available at
					<b><a href="https://arxiv.org/abs/2502.11271" target="_blank">Preprint</a></b>.
				</li>
				<li>
					<strong>[01/2025]&nbsp; <font color="red">New!</font></strong>
					Four papers are accepted to
					<b><a href="https://iclr.cc/Conferences/2025" target="_blank">ICLR 2025</a></b>.
				</li>
				<li>
					<strong>[12/2024]&nbsp; <font color="red">New!</font></strong>
					I am co-organizing the <b><a href="https://mathai2024.github.io/" target="_blank">4th MATH-AI</a></b>
					Workshop at <b><a href="https://neurips.cc/Conferences/2024" target="_blank">NeurIPS 2024</a></b>. See you in Vancouver!
				</li>
				<li>
					<strong>[09/2024]&nbsp; <font color="red">New!</font></strong>
					One paper on self-improving vision-language models is accepted to
					<b><a href="https://neurips.cc/Conferences/2024" target="_blank">NeurPIS 2024</a></b>.
				</li>
				<li>
					<strong>[09/2024]&nbsp; <font color="red">New!</font></strong>
					Excited to have our  <b><a href="https://arxiv.org/abs/2209.09513" target="_blank">NeurIPS 2022</a></b> paper recognized as <b><a href="https://www.paperdigest.org/2024/09/most-influential-nips-papers-2024-09/" target="_blank">Most Influential NIPS Papers</a></b>!
				</li>
				<li>
					<strong>[09/2024]&nbsp; <font color="red">New!</font></strong>
					One paper on self-improving vision-language models is accepted to
					<b><a href="https://neurips.cc/Conferences/2024" target="_blank">NeurPIS 2024</a></b>.
				</li>
				<li>
					<strong>[09/2024]&nbsp; <font color="red">New!</font></strong>
					Three papers are accepted to
					<b><a href="https://2024.emnlp.org/" target="_blank">EMNLP 2024</a></b>. See you in Miami!
				</li>
			</ul>
			<div id="content"> 
				<span id="dots"></span>
				<span id="more">
					<ul style="margin-top: -15px">
						<li>
							<strong>[07/2024]&nbsp; <font color="red">New!</font></strong>
							One paper on visual math problems is accepted to
							<b><a href="https://eccv2024.ecva.net/Conferences/2024" target="_blank">ECCV 2024</a></b>.
						</li>
						<li>
							<strong>[06/2024]&nbsp; <font color="red">New!</font></strong>
							A paper on debugging visual programs is available at
							<b><a href="https://arxiv.org/abs/2406.13444" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[06/2024]&nbsp; <font color="red">New!</font></strong>
							A paper on multi-image understanding is available at
							<b><a href="https://arxiv.org/abs/2406.09411" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[05/2024]&nbsp; <font color="red">New!</font></strong>
							A paper on enhancing LVLMs with self-training is available at
							<b><a href="https://arxiv.org/abs/2405.19716" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[05/2024]&nbsp; <font color="red">New!</font></strong>
							Thrilled to be awarded the
							<b><a href="https://www.bloomberg.com/company/stories/introducing-the-sixth-cohort-of-bloomberg-data-science-ph-d-fellows-2023-2024/" target="_blank">Bloomberg Data Science Ph.D. Fellowship</a></b>! Thanks!
						</li>
						<li>
							<strong>[05/2024]&nbsp; <font color="red">New!</font></strong>
							One paper on advanced quantitative reasoning is accepted to
							<b><a href="https://2024.aclweb.org/" target="_blank">ACL 2024</a> (Findings)</b>.
						</li>
						<li>
							<strong>[05/2024]&nbsp; <font color="red">New!</font></strong>
							Two papers on math reasoning and VLMs are accepted at <b><a href="https://icml.cc/Conferences/2024" target="_blank">ICML 2024</a></b>. See you in Vienna!
						</li>
						<li>
							<strong>[04/2024]&nbsp; <font color="red">New!</font></strong>
							Defended my doctoral dissertation! Thanks to my advisor and committee members!
						</li>
						<li>
							<strong>[03/2024]&nbsp; <font color="red">New!</font></strong>
							I am co-organizing the <b><a href="https://sites.google.com/view/ai4mathworkshopicml2024" target="_blank">AI for Math</a></b>
							Workshop at <b><a href="https://icml.cc/Conferences/2024" target="_blank">ICML 2024</a></b>. See you in Vienna!
						</li>
						<li>
							<strong>[03/2024]&nbsp; <font color="red">New!</font></strong>
							A paper on visual math reasoning with Multi-modal LLMs is available at
							<b><a href="https://arxiv.org/abs/2403.14624" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[02/2024]&nbsp; <font color="red">New!</font></strong>
							A paper on LLMs for advanced quantitative reasoning is available at
							<b><a href="https://arxiv.org/abs/2402.17644" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[01/2024]&nbsp; <font color="red">New!</font></strong>
							Two papers on large multimodal models are accepted to
							<b><a href="https://iclr.cc/Conferences/2024" target="_blank">ICLR 2024</a></b>.
						</li>
						<li>
							<strong>[01/2024]&nbsp; <font color="red">New!</font></strong>
							A paper on model editing for LLMs is available at
							<b><a href="https://arxiv.org/abs/2401.04700" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[01/2024]&nbsp; <font color="red">New!</font></strong>
							Two papers on large multimodal models are accepted to
							<b><a href="https://iclr.cc/Conferences/2024" target="_blank">ICLR 2024</a></b>.
						</li>
						<li>
							<strong>[01/2024]&nbsp; <font color="red">New!</font></strong>
							A paper on model editing for LLMs is available at
							<b><a href="https://arxiv.org/abs/2401.04700" target="_blank">Preprint</a></b>.
						</li>						
						<li>
							<strong>[12/2023]&nbsp; <font color="red">New!</font></strong>
							I am co-organizing the <b><a href="https://sites.google.com/view/tavi-cvpr24/" target="_blank">Tool-Augmented VIsion</a></b>
							Workshop at <b><a href="https://cvpr.thecvf.com/" target="_blank">CVPR 2024</a></b>. See you in Seattle!
						</li>
						<li>
							<strong>[12/2023]&nbsp; <font color="red">New!</font></strong>
							I am attending <b><a href="https://nips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a></b> from Dec 10 to Dec 16. See you in New Orleans!
						</li>
						<li>
							<strong>[12/2023]&nbsp; <font color="red">New!</font></strong>
							Google's <b><a href="https://blog.google/technology/ai/google-gemini-ai/?utm_source=twitter&utm_medium=social&utm_campaign=GDMGemini#performance" target="_blank">Gemini</a></b> benchmarks our <b><a href="https://arxiv.org/abs/2310.02255" target="_blank">MathVista</a></b> for evaluating math reasoning in visual contexts!
						</li>
						<li>
							<strong>[11/2023]&nbsp; <font color="red">New!</font></strong>
							Honored to be covered by <b><a href="https://www.cs.ucla.edu/phd-student-pan-lu-wins-2023-qualcomm-innovation-fellowship/" target="_blank">UCLA CS</a></b> for winning <b><a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-north-america" target="_blank">Qualcomm Innovation Fellowship</a></b>. Thanks!
						</li>
						<li>
							<strong>[10/2023]&nbsp; <font color="red">New!</font></strong>
							The 112-page study on GPT-4V, Bard, and others on visual math reasoning is available
							<b><a href="https://arxiv.org/abs/2310.02255" target="_blank">here</a></b>.
						</li>
						<li>
							<strong>[10/2023]&nbsp; <font color="red">New!</font></strong>
							Honored to serve as PC Chair and co-organize
							<b><a href="https://socalnlp.github.io/symp23/index.html" target="_blank">SoCal NLP 2023</a></b>. See you in LA!
						</li>
						<li>
							<strong>[10/2023]&nbsp; <font color="red">New!</font></strong>
							One paper on mathematical reasoning is accepted to
							<b><a href="https://2023.emnlp.org/" target="_blank">EMNLP 2023</a></b>.
						</li>
						<li>
							<strong>[10/2023]&nbsp; <font color="red">New!</font></strong>
							One paper on mathematical reasoning in visual contexts
							(<b><a href="https://arxiv.org/abs/2310.02255" target="_blank">MathVista</a></b>)
							is submitted to <b>Preprint</b>.
						</li>
						<li>
							<strong>[09/2023]&nbsp; <font color="red">New!</font></strong>
							One paper on tool-augmented LLMs is accepted to
							<b><a href="https://nips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a></b>.
						</li>
						<li>
							<strong>[07/2023]&nbsp; <font color="red">New!</font></strong>
							One paper on a scientific reasoning benchmark
							(<b><a href="https://arxiv.org/abs/2307.10635" target="_blank">SciBench</a></b>)
							is submitted to <b>Preprint</b>.
						</li>
						<li>
							<strong>[07/2023]&nbsp; <font color="red">New!</font></strong>
							I am co-organizing the 3rd <b><a href="https://mathai2023.github.io/" target="_blank">MATH-AI</a></b>
							Workshop at <b>NeurIPS 2023</b>. See you in New Orleans!
						</li>
						<li>
							<strong>[06/2023]&nbsp; <font color="red">New!</font></strong>
							 Excited to receive the <b><a href="https://grad.ucla.edu/funding/financial-aid/funding-for-continuing-students/dissertation-year-fellowship/" target="_blank"> UCLA Dissertation Year Fellowship</a></b>. 
						</li>
						<li>
							<strong>[05/2023]&nbsp; <font color="red">New!</font></strong>
							Honored to deliver a guest lecture for UCLA CS 263: Natural Language Processing. 
							<b><a href="docs/UCLA_CS263_NLP_Guest Lecture_Pan Lu_2023.05.31.pdf" target="_blank">[Slides]</a></b>
						</li>
						<li>
							<strong>[05/2023]&nbsp; <font color="red">New!</font></strong>
							One paper on theorem-driven math question answering
							(<b><a href="https://github.com/wenhuchen/TheoremQA" target="_blank">TheoremQA</a></b>)
							is available at
							<b><a href="https://arxiv.org/abs/2305.12524" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[05/2023]&nbsp; <font color="red">New!</font></strong>
							Honored to deliver a invited talk on tool-augmented LLMs at Google Brain. 
							<b><a href="docs/Chameleon_LLM_Pan_Lu_Google_Brain_2023.05.05.pdf" target="_blank">[Slides]</a></b>
						</li>
						<li>
							<strong>[05/2023]&nbsp; <font color="red">New!</font></strong>
							Delighted to join prestigious <b><a href="https://lightning.ai/" target="_blank">LightingAI</a></b>
							event as invited speaker on Discord. 
						</li>
						<li>
							<strong>[05/2023]&nbsp; <font color="red">New!</font></strong>
							A paper on multimodal procedural planning is available at
							<b><a href="https://arxiv.org/abs/2305.01795" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[05/2023]&nbsp;  <font color="red">New!</font></strong>
							One survey paper on deep learning for mathematical reasoning is accepted to <b>ACL 2023</b></font></b>.
						</li>
						<li>
							<strong>[04/2023]&nbsp; <font color="red">New!</font></strong>
							<b><a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank">LLaMA-Adapter-V2</a></b>, 
							a parameter-efficient visual instruction model</b>, 
							is available at
							<b><a href="https://arxiv.org/abs/2304.15010" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[04/2023]&nbsp; <font color="red">New!</font></strong>
							One tutorial proposal on mathematical reasoning is accepted to
							<b><a href="https://ijcai-23.org/" target="_blank">IJCAI 2023</a></b>.
						</li>
						<li>
							<strong>[04/2023]&nbsp; <font color="red">New!</font></strong>
							One paper on tool augmented LLMs
							(<b><a href="https://chameleon-llm.github.io/" target="_blank">Chameleon</a></b>)
							is available at
							<b><a href="https://arxiv.org/abs/2304.09842" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[04/2023]&nbsp; <font color="red">New!</font></strong>
							Two papers are accepted to
							<b><a href="https://asu-apg.github.io/odrum/" target="_blank">CVPR 2023 O-DRUM Workshop</a></b>.
							<!-- as an <b><font color="red">Oral Presentation</font></b> -->
						</li>
						<li>
							<strong>[03/2023]&nbsp; <font color="red">New!</font></strong>
							One paper on fine-tuning
							<b><a href="https://github.com/facebookresearch/llama" target="_blank">LLaMA</a></b>
							in one hour
							(<b><a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank">LLaMA-Adapter</a></b>)
							is available at
							<b><a href="https://arxiv.org/abs/2303.16199" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[01/2023]&nbsp; <font color="red">New!</font></strong>
							One paper on in-context learning for math reasoning
							(<b><a href="https://tabmwp.github.io/" target="_blank">PromptPG</a></b>)
							is accepted to <b>ICLR 2023</b>.
						</li>
						<li>
							<strong>[12/2022]&nbsp; <font color="red">New!</font></strong>
							A survey paper on deep learning for mathematical reasoning is available at
							<b><a href="https://arxiv.org/abs/2212.10535" target="_blank">Preprint</a></b>.
						</li>
						<li>
							<strong>[12/2022]&nbsp; <font color="red">New!</font></strong>
							One paper is accepted to
							<b><a href="https://knowledge-nlp.github.io/aaai2023/publications.html" target="_blank">AAAI'23 KnowledgeNLP Workshop</a></b> 
							as an <b><font color="red">Oral Presentation</font></b>.
						</li>
						<li>
							<strong>[12/2022]&nbsp; <font color="red">New!</font></strong>
							I am excited to join <b><a href="https://www.microsoft.com/en-us/research/" target="_blank">Microsoft Research</a></b>
							as a research intern!
						</li>
						<li>
							<strong>[10/2022]&nbsp; <font color="red">New!</font></strong>
							Happy to receive the <b>NeurIPS 2022 Scholar Award</b>.
						</li>
						<li>
							<strong>[10/2022]&nbsp; <font color="red">New!</font></strong>
							Two papers on mathematical reasoning are accepted to <b>EMNLP 2022</b>.
						</li>
						<li>
							<strong>[09/2022]&nbsp; <font color="red">New!</font></strong>
							One paper on prompt learning for math reasoning
							(<b><a href="https://tabmwp.github.io/" target="_blank">PromptPG</a></b>)
							is submitted to <b>Preprint</b>.
						</li>
						<li>
							<strong>[09/2022]&nbsp; <font color="red">New!</font></strong>
							One paper on chain-of-thought reasoning for
							<b><a href="https://scienceqa.github.io/" target="_blank">ScienceQA</a></b>
							is accepted to <b>NeurIPS 2022</b>.
						</li>
						<li>
							<strong>[07/2022]&nbsp; <font color="red">New!</font></strong>
							I am co-organizing the 2nd <b><a href="https://mathai2022.github.io/" target="_blank">MATH-AI</a></b>
							Workshop at <b>NeurIPS 2022</b>. See you in New Orleans!
						</li>
						<li>
							<strong>[07/2022]&nbsp; <font color="red">New!</font></strong>
							One paper on socially intelligent agents is accepted to <b>SIGDIAL 2022</b>.
						</li>
						<li>
							<strong>[04/2022]&nbsp;</strong>
							Excited to be listed as a
							<b><a href="https://iclr.cc/Conferences/2022/Reviewers" target="_blank">Highlighted Reviewer</a></b>
							for ICLR 2022.
						</li>
						<li>
						<strong>[03/2022]&nbsp;</strong>  
						I am excited to join <b><a href="https://allenai.org/" target="_blank">Allen Institute for AI (AI2)</a></b>
						as a research intern!
						</li>
						<li>
							<strong>[03/2022]&nbsp;</strong>
							One paper on character animation sampling is submitted to <b>Preprint</b>.
						</li>
						<!-- <li>
							<strong>[02/2022]&nbsp;</strong>
							Data and code for NeurIPS 2021
							<b><a href="https://iconqa.github.io/" target="_blank">IconQA</a></b>
							are released now!
						</li> -->
						<li>
							<strong>[12/2021]&nbsp;</strong>
							Two papers are accepted to <b>AAAI 2022</b></font></b>.
						</li>
						<li>
							<strong>[10/2021]&nbsp;</strong>
							One paper on visual question answering for icon images
							(<b><a href="https://iconqa.github.io/" target="_blank">IconQA</a></b>)
							is accepted to <b>NeurIPS 2021</b>.
						</li>
						<li>
							<strong>[07/2021]&nbsp;</strong>
							I am co-organizing the <b><a href="https://mathai4ed.github.io/" target="_blank">MATHAI4ED</a></b>
							Workshop at <b>NeurIPS 2021</b>. Welcome to participate!
						</li>
						<li>
							<strong>[07/2021]&nbsp; </strong>
							Our workshop proposal for Math AI for Education (MATHAI4ED) is accepted to <b>NeurIPS 2021</b></font>.
							</b>
						</li>
						<li>
							<strong>[05/2021]&nbsp; </strong>
							One paper on interpretable geometry problem solving is accepted to <b>ACL 2021</b> as an <b>
								<font color="red">Oral Presentation</font>.
							</b>
						</li>
						<li>
							<strong>[05/2021]&nbsp; </font>
							</strong> One paper on social relation inference in dialogues is accepted to <b>ACL 2021</b> as an <b>
								<font color="red">Oral Presentation</font>.
							</b>
						</li>
						<li>
							<strong>[03/2021]</strong> One paper on socially intelligent agents is submitted to <b>Preprint</b>.
						</li>
						<!-- <li>
							<strong>[11/2019]</strong> One paper on personalized image caption is accepted to <b>AAAI 2020</b>.
						</li>
						<li>
							<strong>[05/2019]</strong> One paper on knowledge aware image-text matching is accepted to <b>IJCAI
								2019</b> as an <b>
								<font color="red">Oral Presentation</font>.
							</b>
						</li>
						<li>
							<strong>[03/2019]</strong> One paper on dynamic fusion for visual question answering is accepted to
							<b>CVPR 2019</b> as an <b>
								<font color="red">Oral Presentation</font>.
							</b>
						</li> -->
					</ul>
				</span>
			</div>
			<button onclick="showMore()" id="showMoreBtn">Show more</button>
		</div>
	</div>
	<br>

	<!-- Travel -->
	<div class="container">
		<h3 id="Travels" style="">Upcoming Travel</h3>
		<hr>
		<ul>
			<li>December 2 - December 7, 2025:
				<a href="https://neurips.cc/" target="_blank">NeurIPS 2025 at San Diego, California</a>
			</li>
			<!-- <li>June 3 - June 7, 2026:
				<a href="https://www.thecvf.com/" target="_blank">CVPR 2026 at Denver, Colorado</a> (TBD)
			</li> -->
			<!-- <li>July 2 - July 7, 2026:
				<a href="https://2026.aclweb.org/" target="_blank">ACL 2026 at San Diego, California</a> (TBD)
			</li> -->
			<!-- <li>October 2026 (TBD):
				<a href="https://colmweb.org/index.html" target="_blank">COLM 2026 at XXXX (TBD)</a>
			</li> -->
		</ul>
	</div><br>

	<!-- Publications -->
	<div class="container">
		<h3 id="Publications" style="">Selected Publications</h3>
		<hr>
		<!-- <font color="black">(* indicate equal contribution)</font><br><hr> -->
		All publications can be found on my <a href="https://scholar.google.com/citations?user=IyucsdQAAAAJ&hl=en" target="_blank">Google Scholar</a> page.
		<br>
		<br>

		<!-- AgentFlow -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="projects/arxiv25_agentflow/cover.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="">
					<img src="logos/fire.png" id="logo">
					<!-- <img src="logos/fire.png" id="logo"> -->
				</a>
				<b>
					<font color="black">
						In-the-Flow Agentic System Optimization for Effective Planning and Tool Use
					</font>
				</b><br>
				Zhuofeng Li*, Haoxiang Zhang*, Seungju Han, Sheng Liu, Jianwen Xie, Yu Zhang, Yejin Choi, James Zou†, <b>Pan Lu†</b>
				<br>
				<b><a href="https://arxiv.org/abs/2510.05592" target="_blank">arXiv:2510.05592&nbsp;</a></b>
				<a href="https://agentflow.stanford.edu/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://huggingface.co/papers/2510.05592" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://github.com/lupantech/AgentFlow" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/AgentFlow" target="_blank"> <small>[Model]&nbsp;</small></a>
				<a href="https://huggingface.co/spaces/AgentFlow/agentflow" target="_blank"> <small>[Demo]&nbsp;</small></a>
				<a href="https://x.com/lupantech" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://join.slack.com/t/agentflow-co/shared_invite/zt-3f712xngl-LfxS4gmftAeKvcxR3nSkWQ" target="_blank"> <small>[Slack]&nbsp;</small></a>
				<a href="projects/arxiv25_agentflow/bib.txt" target="_blank"> <small>[BibTex]</small></a>
				<!-- <a href="https://github.com/lupantech/AgentFlow" target="_blank"><img src="https://img.shields.io/github/stars/octotools/octotools?style=social" alt="GitHub stars"></a> -->
				<br>(*Equal Contribution) (†Co-senior authors)<br>
			</div>
		</div>
		<hr>

		<!-- IneqMath -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="projects/neurips25_ineqmath/cover.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="">
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">
						Solving Inequality Proofs with Large Language Models
					</font>
				</b><br>
				Jiayi Sheng*, Luna Lyu*, Jikai Jin, Tony Xia, Alex Gu, James Zou†, <b>Pan Lu†</b>
				<br>
				<b><a href="https://arxiv.org/abs/2506.07927" target="_blank">NeurIPS 2025&nbsp;</a></b>
				<a href="https://ineqmath.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2506.07927" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://github.com/lupantech/ineqmath" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/datasets/AI4Math/IneqMath" target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="https://huggingface.co/spaces/AI4Math/IneqMath-Leaderboard" target="_blank"> <small>[Submission]&nbsp;</small></a>
				<a href="https://x.com/lupantech/status/1932866286427779586" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="projects/neurips25_ineqmath/bib.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/lupantech/ineqmath" target="_blank"><img src="https://img.shields.io/github/stars/lupantech/ineqmath?style=social" alt="GitHub stars"></a>
				<br>(*Equal Contribution) (†Co-senior authors)<br>
				<a id="award" href="https://knowledge-nlp.github.io/naacl2025/index.html" target="_blank">
					<font color="red"><b>🏆 Spotlight (Top 3%)</b> </font>
				</a>
			</div>
		</div>
		<hr>

		<!-- OctoTools -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="projects/arxiv25_octotools/cover.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="">
					<img src="logos/fire.png" id="logo">
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">
						OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning
					</font>
				</b><br>
				<b>Pan Lu*</b>, Bowen Chen*, Sheng Liu*, Rahul Thapa, Joseph Boen, James Zou
				<br>
				<b><a href="https://arxiv.org/abs/2502.11271" target="_blank">arXiv:2502.11271&nbsp;</a></b>
				<a href="https://octotools.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2502.11271" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<!-- <a href="https://arxiv.org/pdf/2502.11271" target="_blank"> <small>[PDF]&nbsp;</small></a> -->
				<a href="https://github.com/octotools/octotools" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://pypi.org/project/octotoolkit/" target="_blank"> <small>[Package]&nbsp;</small></a>
				<a href="https://huggingface.co/spaces/OctoTools/octotools" target="_blank"> <small>[Demo]&nbsp;</small></a>
				<a href="https://www.youtube.com/watch?v=4828sGfx7dk&ab_channel=DiscoverAI" target="_blank"> <small>[YouTube]&nbsp;</small></a>
				<a href="https://x.com/lupantech/status/1892260474320015861" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://join.slack.com/t/octotools/shared_invite/zt-3485ikfas-zMTbFbuodJmET~R6KXHEGw" target="_blank"> <small>[Slack]&nbsp;</small></a>
				<a href="projects/arxiv25_octotools/bib.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/octotools/octotools" target="_blank"><img src="https://img.shields.io/github/stars/octotools/octotools?style=social" alt="GitHub stars"></a>
				<br>(*Equal Contribution)<br>
				<a id="award" href="https://knowledge-nlp.github.io/naacl2025/index.html" target="_blank">
					<font color="red"><b>🏆 Best Paper Award, KnowledgeNLP Workshop at NAACL 2025</b> </font>
				</a>
			</div>
		</div>
		<hr>

		<!-- TextGrad -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="projects/nature25_textgrad/cover.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="">
					<img src="logos/fire.png" id="logo">
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">
						Optimizing generative AI by backpropagating language model feedback
					</font>
				</b><br>
				Mert Yuksekgonul*, Federico Bianchi*, Joseph Boen*, Sheng Liu*, <b>Pan Lu*</b>, Zhi Huang*, Carlos Guestrin, James Zou
				<br>
				<b><a href="https://www.nature.com/articles/s41586-025-08661-4" target="_blank">Nature 639, 609–616 (2025)&nbsp;</a></b>
				<a href="https://textgrad.com/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://www.nature.com/articles/s41586-025-08661-4" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<!-- <a href="https://arxiv.org/pdf/2406.07496.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a> -->
				<a href="https://github.com/zou-group/textgrad" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://www.youtube.com/watch?v=Qks4UEsRwl0&ab_channel=DiscoverAI" target="_blank"> <small>[YouTube]&nbsp;</small></a>
				<a href="https://textgrad.readthedocs.io/en/latest/index.html" target="_blank"> <small>[Documentation]&nbsp;</small></a>
				<a href="projects/nature25_textgrad/bib.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/zou-group/textgrad" target="_blank"><img src="https://img.shields.io/github/stars/zou-group/textgrad?style=social" alt="GitHub stars"></a>
				<br>
				
				(*Equal Contribution)<br>
			</div>
		</div>
		<hr>

		<!-- Protein-LLM -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="projects/arxiv25_protein-llm/cover.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">
						Protein Large Language Models: A Comprehensive Survey
					</font>
				</b><br>
				Yijia Xiao, Wanjia Zhao, Junkai Zhang, Yiqiao Jin, Han Zhang, Zhicheng Ren, Renliang Sun, Haixin Wang, Guancheng Wan, <b>Pan Lu</b>, Xiao Luo, Yu Zhang, James Zou, Yizhou Sun, Wei Wang
				<br>
				<b><a href="https://arxiv.org/abs/2502.17504" target="_blank">Preprint&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2502.17504" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2502.17504.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/Yijia-Xiao/Protein-LLM-Survey" target="_blank"> <small>[Tutorial]&nbsp;</small></a>
				<a href="https://x.com/YIJIA_XIAO_/status/1895156151622263034" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="projects/arxiv25_protein-llm/bib.txt" target="_blank"> <small>[BibTex]</small></a>
				<!-- <a href="https://github.com/Yijia-Xiao/Protein-LLM-Survey" target="_blank"><img src="https://img.shields.io/github/stars/Yijia-Xiao/Protein-LLM-Survey?style=social" alt="GitHub stars"></a> -->
			</div>
		</div>
		<hr>

		<!-- VISCO -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="projects/cvpr25_visco/cover.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">
						VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning
					</font>
				</b><br>
				Xueqing Wu, Yuheng Ding, Bingxuan Li, <b>Pan Lu</b>, Da Yin, Kai-Wei Chang, Nanyun Peng
				<br>
				<b><a href="https://arxiv.org/abs/2412.02172" target="_blank">CVPR 2025&nbsp;</a></b>
				<a href="https://visco-benchmark.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2412.02172" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2412.02172.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/PlusLabNLP/VISCO" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/datasets/uclanlp/VISCO" target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="projects/cvpr25_visco/bib.txt" target="_blank"> <small>[BibTex]</small></a>
			</div>
		</div>
		<hr>

		<!-- ChemAgent -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="projects/iclr25_chemagent/cover.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">
						ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning
					</font>
				</b><br>
				Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, <b>Pan Lu</b>, Zhuosheng Zhang, Yilun Zhao, Arman Cohan, Mark Gerstein
				<br>
				<b><a href="https://arxiv.org/abs/2501.06590" target="_blank">ICLR 2025&nbsp;</a></b>
				<!-- <a href="https://chemagent.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a> -->
				<a href="https://arxiv.org/abs/2501.06590" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2501.06590.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/gersteinlab/chemagent" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://www.marktechpost.com/2025/01/17/chemagent-enhancing-large-language-models-for-complex-chemical-reasoning-with-dynamic-memory-frameworks/" target="_blank"> <small>[News]&nbsp;</small></a>
				<a href="projects/iclr25_chemagent/bib.txt" target="_blank"> <small>[BibTex]</small></a>
			</div>
		</div>
		<hr>

		<!-- MRAG -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="projects/iclr25_mrag/cover.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">
						MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models
					</font>
				</b><br>
				Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, <b>Pan Lu</b>, Kai-Wei Chang, Nanyun Peng
				<br>
				<b><a href="https://arxiv.org/abs/2410.08182" target="_blank">ICLR 2025&nbsp;</a></b>
				<a href="https://mragbench.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2410.08182" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2410.08182.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/mragbench/MRAG-Bench" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/datasets/uclanlp/MRAG-Bench" target="_blank"> <small>[HF Dataset]&nbsp;</small></a>
				<a href="projects/iclr25_mrag/bib.txt" target="_blank"> <small>[BibTex]</small></a>
			</div>
		</div>
		<hr>

		<!-- MMSearch -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="projects/iclr25_mmsearch/cover.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">
						MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines
					</font>
				</b><br>
				Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, <b>Pan Lu</b>, Zehui Chen, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, Hongsheng Li
				<br>
				<b><a href="https://arxiv.org/abs/2409.12959" target="_blank">ICLR 2025&nbsp;</a></b>
				<a href="https://mmsearch.github.io" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2409.12959" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2409.12959.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://huggingface.co/papers/2409.12959" target="_blank"> <small>[Hugging Face]&nbsp;</small></a>
				<a href="https://github.com/CaraJ7/MMSearch" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/datasets/CaraJ/MMSearch" target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="projects/iclr25_mmsearch/bib.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/CaraJ7/MMSearch" target="_blank"><img src="https://img.shields.io/github/stars/CaraJ7/MMSearch?style=social" alt="GitHub stars"></a>
			</div>
		</div>
		<hr>

		<!-- MuirBench -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="projects/iclr25_muirbench/cover.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">
						MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding
					</font>
				</b><br>
				Fei Wang*, Xingyu Fu*, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, <b>Pan Lu</b>, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, Muhao Chen
				<br>
				<b><a href="https://arxiv.org/abs/2406.09411" target="_blank">ICLR 2025&nbsp;</a></b>
				<a href="https://muirbench.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2406.09411" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2406.09411.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://huggingface.co/papers/2406.09411" target="_blank"> <small>[Hugging Face]&nbsp;</small></a>
				<a href="https://github.com/muirbench/MuirBench" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH" target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="https://x.com/lupantech/status/1806021467836186911" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="projects/iclr25_muirbench/bib.txt" target="_blank"> <small>[BibTex]</small></a>
				<br>
				(*Equal Contribution)<br>
			</div>
		</div>
		<hr>

		<!-- STIC -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/neurips24_stic.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="">
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">
						Enhancing Large Vision Language Models with Self-Training on Image Comprehension
					</font>
				</b><br>
				Yihe Deng*, <b>Pan Lu</b>*, Fan Yin, Ziniu Hu, Sheng Shen, Quanquan Gu, James Zou, Kai-Wei Chang, Wei Wang
				<br>
				<b><a href="https://neurips.cc/Conferences/2024" target="_blank">NeurIPS 2024&nbsp;</a></b>
				<a href="https://stic-lvlm.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2405.19716" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2405.19716.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://huggingface.co/STIC-LVLM" target="_blank"> <small>[Hugging Face]&nbsp;</small></a>
				<a href="https://github.com/yihedeng9/STIC" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/collections/STIC-LVLM/stic-models-6658e7cdfac70f94c2a38af1" target="_blank"> <small>[Model]&nbsp;</small></a>
				<a href="https://huggingface.co/collections/STIC-LVLM/stic-data-6658e7f93aa5d4bb34ef140b" target="_blank"> <small>[Data]&nbsp;</small></a>
				<!-- <a href="https://twitter.com/_akhaliq/status/1771019526265618889" target="_blank"> <small>[Coverage]&nbsp;</small></a> -->
				<!-- <a href="https://huggingface.co/papers/240X.XXXXX" target="_blank"> <small>[Daily Papers]&nbsp;</small></a> -->
				<a href="bibs/neurips24_stic.txt" target="_blank"> <small>[BibTex]</small></a>
				<br>
				(*Equal Contribution)<br>
			</div>
		</div>
		<hr>

		<!-- Model Editing -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/emnlp24_modelediting.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue</font>
				</b><br>
				Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, <b>Pan Lu</b>, Zhen-Hua Ling, Kai-Wei Chang, Nanyun Peng
				<br>
				<b><a href="https://arxiv.org/abs/2401.04700" target="_blank">EMNLP 2024&nbsp;</a></b>
				<!-- <a href="https://mathvista.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a> -->
				<a href="https://arxiv.org/abs/2401.04700" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2401.04700.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/JasonForJoy/Model-Editing-Hurt" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/JasonForJoy/status/1744932437006885060" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<!-- <a href="" target="_blank"> <small>[Coverage]&nbsp;</small></a> -->
				<a href="bibs/emnlp24_modelediting.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<a id="award" >
					<font color="red"><b>🏆 Best Paper Nomination, EMNLP 2024</b> </font>
				</a> 
				<br>
			</div>
		</div>
		<hr>

		<!-- VDebugger -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/emnlp24_vdebugger.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">
						VDebugger: Harnessing Execution Feedback for Debugging Visual Programs
					</font>
				</b><br>
				Xueqing Wu, Zongyu Lin, Songyan Zhao, Te-Lin Wu, <b>Pan Lu</b>, Nanyun Peng, Kai-Wei Chang
				<br>
				<b><a href="https://arxiv.org/abs/2406.13444" target="_blank">EMNLP 2024 (Findings)&nbsp;</a></b>
				<a href="https://shirley-wu.github.io/vdebugger/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2406.13444" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2406.13444.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/shirley-wu/vdebugger/" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/VDebugger" target="_blank"> <small>[Model]&nbsp;</small></a>
				<a href="https://huggingface.co/VDebugger" target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="https://x.com/lupantech/status/1805396589785268557" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<!-- <a href="https://huggingface.co/papers/240X.XXXXX" target="_blank"> <small>[Daily Papers]&nbsp;</small></a> -->
				<a href="bibs/emnlp24_vdebugger.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- MPP -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/emnlp24_mpp.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://github.com/YujieLu10/TIP">
					<img src="logos/fire.png" id="logo">
				</a> -->
				<b>
					<font color="black">Multimodal Procedural Planning via Dual Text-Image Prompting</font>
				</b><br>
				Yujie Lu, <b>Pan Lu</b>, Zhiyu Chen, Wanrong Zhu, Xin Eric Wang, William Yang Wang<br>
				<b><a href="https://arxiv.org/abs/2305.01795" target="_blank">EMNLP 2024 (Findings)&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2305.01795" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/arxiv23_mpp.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/YujieLu10/TIP" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/yujielu_10/status/1653928324408958977" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/_akhaliq/status/1653922454933319680" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/emnlp24_mpp.txt" target="_blank"> <small>[BibTex]</small></a>
				<!-- <a href="https://github.com/YujieLu10/TIP" target="_blank"><img src="https://img.shields.io/github/stars/YujieLu10/TIP?style=social" alt="GitHub stars"></a> -->
			</div>
		</div>
		<hr>
		
		<!-- MathVerse -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv24_mathverse.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">
						MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
					</font>
				</b><br>
				Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, <b>Pan Lu</b>, Kai-Wei Chang, Peng Gao, Hongsheng Li
				<br>
				<b><a href="https://eccv2024.ecva.net/Conferences/2024" target="_blank">ECCV 2024&nbsp;</a></b>
				<a href="https://mathverse-cuhk.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2403.14624" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2403.14624.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/ZrrSkywalker/MathVerse" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/datasets/AI4Math/MathVerse" target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="https://mathverse-cuhk.github.io/#visualization" target="_blank"> <small>[Visualization]&nbsp;</small></a>
				<a href="https://twitter.com/_akhaliq/status/1771019526265618889" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="https://huggingface.co/papers/2403.14624" target="_blank"> <small>[Daily Papers]&nbsp;</small></a>
				<a href="bibs/arxiv24_mathverse.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/ZrrSkywalker/MathVerse" target="_blank"><img src="https://img.shields.io/github/stars/ZrrSkywalker/MathVerse?style=social" alt="GitHub stars"></a>
			</div>
		</div>
		<hr>

		<!-- QRData -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/acl24_qrdata.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">
						Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data
					</font>
				</b><br>
				Xiao Liu, Zirui Wu, Xueqing Wu, <b>Pan Lu</b>, Kai-Wei Chang, Yansong Feng
				<br>
				<b><a href="https://2024.aclweb.org/" target="_blank">ACL 2024&nbsp;</a> (Findings)</b>
				<a href="https://xxxiaol.github.io/QRData/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2402.17644" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2402.17644.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/xxxiaol/QRData" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://github.com/xxxiaol/QRData" target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="https://twitter.com/xxxxiaol/status/1763061050868674890" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/acl24_qrdata.txt" target="_blank"> <small>[BibTex]</small></a>
			</div>
		</div>
		<hr>

		<!-- SciBench -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/icml24_scibench.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="">
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</font>
				</b><br>
				Xiaoxuan Wang*, Ziniu Hu*, <strong>Pan Lu</strong>*, Yanqiao Zhu*, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang
				<br>
				<b><a href="https://arxiv.org/abs/2307.10635" target="_blank">ICML 2024&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2307.10635" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/icml24_scibench.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/mandyyyyii/scibench" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/icml24_scibench.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/mandyyyyii/scibench" target="_blank"><img src="https://img.shields.io/github/stars/mandyyyyii/scibench?style=social" alt="GitHub stars"></a>
				<br>
				(*Equal Contribution)<br>
				<a id="award" href="https://www.nature.com/articles/d41586-023-03507-3" target="_blank">
					<font color="red"><b>Nature News Feature</b> </font>
				</a>
				(15 November 2023)
			</div>
		</div>
		<hr>

		<!-- SPHINX-X -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/icml24_sphinx.png" style="" alt="">
			</div>
			<div class="col-md-9">

				<b>
					<font color="black">SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
					</font>
				</b><br>
				Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, <b>Pan Lu</b>, Hongsheng Li, Yu Qiao
				<br>
				<b><a href="https://arxiv.org/abs/2402.05935" target="_blank">ICML 2024&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2402.05935" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2402.05935.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://llama2-accessory.readthedocs.io/en/latest/" target="_blank"> <small>[Doc]&nbsp;</small></a>
				<a href="https://huggingface.co/papers/2402.05935" target="_blank"> <small>[Hugging Face]&nbsp;</small></a>
				<a href="" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/_akhaliq/status/1755794618019368978" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/icml24_sphinx.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory" target="_blank"><img src="https://img.shields.io/github/stars/Alpha-VLLM/LLaMA2-Accessory?style=social" alt="GitHub stars"></a>
			</div>
		</div>
		<hr>

		<!-- MathVista -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/iclr24_mathvista.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://mathvista.github.io/">
					<img src="logos/fire.png" id="logo"><img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts</font>
				</b><br>
				<b>Pan Lu</b>, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao
				<br>
				<b><a href="https://arxiv.org/abs/2310.02255" target="_blank">ICLR 2024&nbsp;</a></b>
				<a href="https://mathvista.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2310.02255" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/iclr24_mathvista.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/lupantech/MathVista" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/datasets/AI4Math/MathVista" target="_blank"> <small>[Dataset]&nbsp;</small></a>
				<a href="https://mathvista.github.io/#leaderboard" target="_blank"> <small>[Leaderboard]&nbsp;</small></a>
				<a href="https://mathvista.github.io/#visualization" target="_blank"> <small>[Visualize]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1717313355780964608" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/iclr24_mathvista.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/lupantech/MathVista" target="_blank"><img src="https://img.shields.io/github/stars/lupantech/MathVista?style=social" alt="GitHub stars"></a>
				
				<br>
				<a id="award" href="https://resources.paperdigest.org/2025/03/most-influential-iclr-papers-2025-03-version/" target="_blank">
					<font id="award" color="red"><b>🏆 Most Influential ICLR Papers </b> </font> 
				</a>
				(Top-15 cited paper at ICLR-24)
				<br>
				<a id="award" href="" target="_blank">
					<font color="red"><b>🏆 Oral Presentation (1.2%)</b> </font>
				</a> (85 in 7304 submissions) 
				<br>
				<!-- <a id="award" href="https://cryptorank.io/news/feed/d1046-ais-roblem-solving-examined-with-mathvista" target="_blank">
					<font color="red"><b>CryptoRank News Feature</b> </font>
				</a> (29 October 2023) 
				<br> -->
			</div>
		</div>
		<hr>

		<!-- LLaMA-Adapter -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/iclr24_llama.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://github.com/OpenGVLab/LLaMA-Adapter">
					<img src="logos/fire.png" id="logo"><img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention					</font>
				</b><br>
				Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, <b>Pan Lu</b>, Hongsheng Li, Peng Gao, Yu Qiao
				<br>
				<b><a href="https://arxiv.org/abs/2303.16199" target="_blank">ICLR 2024&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2303.16199" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/iclr24_llama.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1640899600281395200" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/_akhaliq/status/1640884275439628288" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/iclr24_llama.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank"><img src="https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter?style=social" alt="GitHub stars"></a>
				<br>
				<a id="award" href="https://lightning.ai/pages/community/article/understanding-llama-adapters/" target="_blank">
					<font color="red"><b>LightningAI Blog Feature</b> </font>
				</a> (14 April 2023) <br>
			</div>
		</div>
		<hr>

		<!-- Chameleon -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/neurips23_chameleon.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://chameleon-llm.github.io/">
					<img src="logos/fire.png" id="logo"><img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</font>
				</b><br>
				<b>Pan Lu</b>, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao
				<br>
				<b><a href="https://arxiv.org/abs/2304.09842" target="_blank">NeurIPS 2023&nbsp;</a></b>
				<a href="https://chameleon-llm.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://arxiv.org/abs/2304.09842" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/neurips23_chameleon.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/lupantech/chameleon-llm" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1648879085115052033" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/_akhaliq/status/1648851856930533378" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/neurips23_chameleon.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/lupantech/chameleon-llm" target="_blank"><img src="https://img.shields.io/github/stars/lupantech/chameleon-llm?style=social" alt="GitHub stars"></a>
				<br>
				<a id="award" href="https://alphasignalai.beehiiv.com/p/weeks-top-5-ai-papers?utm_source=alphasignalai.beehiiv.com&utm_medium=newsletter&utm_campaign=this-week-s-top-5-ai-papers" target="_blank">
					<font color="red"><b>🏆 Best Weekly AI Paper</b> </font>
				</a> (by AlphaSignal, 1st in 1682, 0.06%) <br>
				<a id="award" href="https://github.com/jacobmarks/awesome-neurips-2023/tree/main" target="_blank">
					<font color="red"><b>🏆 Awesome NeurIPS 2023 Papers</b> </font>
				</a> (40 in 3584, 0.01%) <br>
				<a id="award" href="https://medium.com/voxel51/neurips-2023-survival-guide-2f957d5b07c9" target="_blank">
					<font color="red"><b>🏆 NeurIPS 2023 Top 10 Multimodal ML Papers </b> </font>
				</a>

			</div>
		</div>
		<hr>

		<!-- KokoMind -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/kokomind2023.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://chats-lab.github.io/KokoMind/">
					<img src="logos/fire.png" id="logo">
				</a> -->
				<b>
					<font color="black">KokoMind: Can LLMs Understand Social Interactions?</font>
				</b><br>
				Weiyan Shi*, Liang Qiu*, Dehong Xu, Pengwei Sui, <b>Pan Lu</b>, Zhou Yu<br>
				<a href="https://chats-lab.github.io/KokoMind/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/CHATS-lab/KokoMind/tree/main" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1676708418697203713" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/shi_weiyan/status/1676697071674601473" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/kokomind2023.txt" target="_blank"> <small>[BibTex]</small></a><br>
				(*Equal Contribution)
			</div>
		</div>
		<hr>

		<!-- TheoremQA -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/emnlp23_theoremqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://github.com/wenhuchen/TheoremQA">
					<img src="logos/fire.png" id="logo">
				</a> -->
				<b>
					<font color="black">TheoremQA: A Theorem-driven Question Answering Dataset</font>
				</b><br>
				Wenhu Chen, Ming Yin, Max Ku, <b>Pan Lu</b>, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia<br>
				<b><a href="https://2023.emnlp.org/" target="_blank">EMNLP 2023&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2305.12524" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/emnlp23_theoremqa.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/wenhuchen/TheoremQA" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://twitter.com/WenhuChen/status/1660832837715611648" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/emnlp23_theoremqa.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/wenhuchen/TheoremQA" target="_blank"><img src="https://img.shields.io/github/stars/wenhuchen/TheoremQA?style=social" alt="GitHub stars"></a>
			</div>
		</div>
		<hr>

		<!-- Ark -->
		<!-- <div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv23_ark.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">ArK: Augmented Reality with Knowledge Emergent Infrastructure</font>
				</b><br>
				Abhinav Gupta*, Qiuyuan Huang*, Jae Sung Park*, <strong>Pan Lu</strong>*, Paul N. Bennett, Ran Gong, Subhojit Som, Baolin Peng, Owais Khan Mohammed, Christopher Pal, Yejin Choi, Jianfeng Gao<br>
				<b><a href="https://arxiv.org/abs/2305.00970" target="_blank">arXiv:2305.00970&nbsp;</a></b>
				<a href="papers/arxiv23_ark.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://www.microsoft.com/en-us/research/uploads/prod/2023/04/ArK.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="bibs/arxiv23_ark.txt" target="_blank"> <small>[BibTex]</small></a><br>
				(*Equal Contribution)
			</div>
		</div>
		<hr> -->

		<!-- LLaMA-Adapter-V2 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv23_llamav2.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://github.com/OpenGVLab/LLaMA-Adapter">
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</font>
				</b><br>
				Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, <b>Pan Lu</b>, Conghui He, Xiangyu Yue, Hongsheng Li, Yu Qiao
				<br>
				<b><a href="https://arxiv.org/abs/2304.15010" target="_blank">arXiv:2304.15010&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2304.15010" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/arxiv23_llamav2.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="http://llama-adapter.opengvlab.com/" target="_blank"> <small>[Gradio]&nbsp;</small></a>
				<a href="http://imagebind-llm.opengvlab.com/" target="_blank"> <small>[Gradio-Multimodal]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1652022897563795456" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://www.youtube.com/watch?v=GAJyWkkSd8M&ab_channel=BruinTech" target="_blank"> <small>[YouTube]&nbsp;</small></a>
				<!-- <a href="https://twitter.com/_akhaliq/status/1640884275439628288" target="_blank"> <small>[Coverage]&nbsp;</small></a> -->
				<a href="bibs/arxiv23_llamav2.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank"><img src="https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter?style=social" alt="GitHub stars"></a>
			</div>
		</div>
		<hr>

		<!-- Survey -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/acl23_dl4math.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://github.com/lupantech/dl4math">
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">A Survey of Deep Learning for Mathematical Reasoning</font>
				</b><br>
				<b>Pan Lu</b>, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang<br>
				<b><a href="https://arxiv.org/abs/2212.10535" target="_blank">ACL 2023&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2212.10535" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/acl23_dl4math.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/lupantech/dl4math" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="posters/acl23_dl4math.pdf" target="_blank"> <small>[Poster]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1605400505697841155" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="https://twitter.com/papers_daily/status/1607232372289724416" target="_blank"> <small>[Coverage]&nbsp;</small></a>
				<a href="bibs/acl23_dl4math.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/lupantech/dl4math" target="_blank"><img src="https://img.shields.io/github/stars/lupantech/dl4math?style=social" alt="GitHub stars"></a>
				<br>
				<a id="award" href="https://resources.paperdigest.org/2025/03/most-influential-arxiv-artificial-intelligence-papers-2025-03-version//" target="_blank">
					<font id="award" color="red"><b>🏆 Most Influential ArXiv (Artificial Intelligence) Papers </b> </font> 
				</a>
				(Top-25 cited paper at arXiv-22)
			</div>
		</div>
		<hr>

		<!-- TabMWP -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/iclr23_promptpg.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://promptpg.github.io/">
					<!-- <img src="logos/promptpg.png" id="logo"> -->
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical
						Reasoning
					</font>
				</b><br>
				<b>Pan Lu</b>, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark,
				Ashwin Kalyan<br>
				<b><a href="https://arxiv.org/abs/2209.14610" target="_blank">ICLR 2023&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2209.14610" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/iclr23_promptpg.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://promptpg.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://drive.google.com/drive/folders/1IYgGrY9agwF_qQlh4WNRG_4WF_ggTkvG?usp=sharing"
					target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="https://github.com/lupantech/PromptPG" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://promptpg.github.io/explore.html" target="_blank"> <small>[Explore]&nbsp;</small></a>
				<a href="https://promptpg.github.io/leaderboard.html" target="_blank">
					<small>[Leaderboard]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1623039527026831361" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/iclr23_promptpg.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/lupantech/PromptPG" target="_blank"><img src="https://img.shields.io/github/stars/lupantech/PromptPG?style=social" alt="GitHub stars"></a>
			</div>
		</div>
		<hr>

		<!-- ScienceQA -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/neurips22_scienceqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<a href="https://scienceqa.github.io/">
					<!-- <img src="logos/scienceqa.png" id="logo"> -->
					<img src="logos/fire.png" id="logo">
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question
						Answering</font>
				</b><br>
				<b>Pan Lu</b>, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter
				Clark,
				Ashwin Kalyan<br>
				<b><a href="https://nips.cc/" target="_blank">NeurIPS 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2209.09513" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/neurips22_scienceqa.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://scienceqa.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://drive.google.com/drive/u/1/folders/1w8imCXWYn2LxajmGeGH_g5DaL2rabHev" target="_blank">
					<small>[Data]&nbsp;</small></a>
				<a href="https://huggingface.co/datasets/derek-thomas/ScienceQA" target="_blank"> <small>[Huggingface]&nbsp;</small></a>
				<a href="https://github.com/lupantech/ScienceQA" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://scienceqa.github.io/explore.html" target="_blank"> <small>[Explore]&nbsp;</small></a>
				<a href="https://scienceqa.github.io/leaderboard.html" target="_blank">
					<small>[Leaderboard]&nbsp;</small></a>
				<a href="https://twitter.com/lupantech/status/1570828580346802178" target="_blank"> <small>[Twitter]&nbsp;</small></a>
				<a href="bibs/neurips22_scienceqa.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/lupantech/ScienceQA" target="_blank"><img src="https://img.shields.io/github/stars/lupantech/ScienceQA?style=social" alt="GitHub stars"></a>
				<br>
				<a id="award" href="https://resources.paperdigest.org/2024/09/most-influential-nips-papers-2024-09/" target="_blank">
					<font id="award" color="red"><b>🏆 Most Influential NIPS Papers </b> </font> 
				</a>
				(Top-15 cited paper at NeurIPS-22)
			</div>
		</div>
		<hr>

		<!-- Lila -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/emnlp22_lila.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://lila.apps.allenai.org/" style="font-size:20pt">
					📜
				</a> -->
				<b>
					<font color="black">LILA: A Unified Benchmark for Mathematical Reasoning</font>
				</b><br>
				Swaroop Mishra*, Matthew Finlayson*, <b>Pan Lu</b>, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay
				Rajpurohit,
				Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin K. Kalyan
				<br>
				<b><a href="https://2022.emnlp.org/" target="_blank">EMNLP 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2210.17517" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2210.17517.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://lila.apps.allenai.org/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/allenai/Lila" target="_blank"> <small>[Data]&nbsp;</small></a>
				<a href="https://github.com/allenai/Lila" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="https://huggingface.co/allenai/bhaskara" target="_blank">
					<small>[Huggingface]&nbsp;</small></a>
				<a href="bibs/emnlp22_lila.txt" target="_blank"> <small>[BibTex]</small></a>
				<br>
				(*Equal Contribution)
			</div>
		</div>
		<hr>

		<!-- UniGeo -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/emnlp22_unigeo.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical
						Expression</font>
				</b><br>
				Jiaqi Chen, Tong Li, Jinghui Qin, <b>Pan Lu</b>, Liang Lin, Chongyu Chen and Xiaodan Liang
				<br>
				<b><a href="https://2022.emnlp.org/" target="_blank">EMNLP 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2212.02746" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/emnlp22_unigeo.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://github.com/chen-judge/UniGeo" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/emnlp22_unigeo.txt" target="_blank"> <small>[BibTex]</small></a>
			</div>
		</div>
		<hr>

		<!-- Mind -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv21_mind.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Towards Socially Intelligent Agents with Mental State Transition and Human
						Utility</font>
				</b><br>
				<a>Liang Qiu*, Yizhou Zhao*, Yuan Liang, <b>Pan Lu</b>, Weiyan Shi, Zhou Yu,
					Song-Chun Zhu</a><br>
				<b><a href="https://2022.sigdial.org/" target="_blank">SIGDIAL 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2103.07011" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/sigdial22_mind.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="bibs/sigdial22_mind.txt" target="_blank"> <small>[BibTex]</small></a><br>
				(*Equal Contribution)
			</div>
		</div>
		<hr>

		<!-- Animation -->
		<!-- <div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv22_animation.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Triangular Character Animation Sampling with Motion, Emotion, and Relation
					</font>
				</b><br>
				<a>Yizhou Zhao, Liang Qiu, Wensi Ai, <b>Pan Lu</b>, Song-Chun Zhu</a><br>
				<b><a href="https://arxiv.org/abs/2203.04930" target="_blank">arXiv:2203.04930&nbsp;</a></b>
				<a href="https://arxiv.org/pdf/2203.04930.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/arxiv22_animation.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr> -->

		<!-- AAAI22 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaai22_tangram.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Learning from the Tangram to Solve Mini Visual Tasks</font>
				</b><br>
				<a>Yizhou Zhao, Liang Qiu, <b>Pan Lu</b>, Feng Shi, Tian Han, Song-Chun Zhu</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-22/" target="_blank">AAAI 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2112.06113" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://arxiv.org/pdf/2112.06113.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<!-- <a href="" target="_blank"> <small>[Project]&nbsp;</small></a> -->
				<a href="https://github.com/yizhouzhao/Tangram" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/aaai22_tangram.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- AAAI22 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaai22_valuenet.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">ValueNet: A New Dataset for Human Value Driven Dialogue System</font>
				</b><br>
				<a>Liang Qiu, Yizhou Zhao, Jinchao Li, <b>Pan Lu</b>, Baolin Peng, Jianfeng Gao, Song-Chun Zhu</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-22/" target="_blank">AAAI 2022&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2112.06346" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/aaai22_valuenet.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://liang-qiu.github.io/ValueNet/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<!-- <a href="" target="_blank"> <small>[Code]&nbsp;</small></a> -->
				<a href="bibs/aaai22_valuenet.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<!-- <font color="firebrick"><b>Oral Presentation</b></font> -->
			</div>
		</div>
		<hr>

		<!-- GenMotion -->
		<!-- <div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/arxiv21_genmotion.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">GenMotion: Data-driven Motion Generators for Real-time Animation Synthesis
					</font>
				</b><br>
				<a>Yizhou Zhao, Wensi Ai, Liang Qiu, <b>Pan Lu</b>, Feng Shi, Tian Han, Song-Chun Zhu</a><br>
				<b><a href="https://arxiv.org/abs/2112.06060" target="_blank">arXiv:2112.06060&nbsp;</a></b>
				<a href="https://arxiv.org/pdf/2112.06060.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/arxiv21_genmotion.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr> -->

		<!-- NeurIPS21 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/neurips21_iconqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://iconqa.github.io/">
					<img src="logos/iconqa.png" id="logo">
				</a> -->
				<b>
					<font color="black">IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language
						Reasoning</font>
				</b><br>
				<a><b>Pan Lu</b>, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang,
					Song-Chun Zhu</a><br>
				<b><a href="https://nips.cc/Conferences/2021" target="_blank">NeurIPS 2021&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2110.13214" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/neurips21_iconqa.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://iconqa.github.io/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/lupantech/IconQA" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/neurips21_iconqa.txt" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<font color="firebrick"><b>Datasets and Benchmarks Track</b></font>
			</div>
		</div>
		<hr>

		<!-- ACL21 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/acl21_gps.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<!-- <a href="https://lupantech.github.io/inter-gps/">
					<img src="logos/intergps.svg" id="logo">
				</a> -->
				<a href="https://lupantech.github.io/inter-gps/">
					<!-- <img src="logos/scienceqa.png" id="logo"> -->
					<img src="logos/fire.png" id="logo">
				</a>
				<b>
					<font color="black">Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and
						Symbolic Reasoning</font>
				</b><br>
				<a><b>Pan Lu</b>*, Ran Gong*, Shibiao Jiang*, Liang Qiu, Siyuan Huang,
					Xiaodan Liang, Song-Chun Zhu</a><br>
				<b><a href="https://2021.aclweb.org/" target="_blank">ACL 2021&nbsp;</a></b>
				<a href="https://aclanthology.org/2021.acl-long.528/" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/acl21_intergps.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="https://lupantech.github.io/inter-gps/" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://github.com/lupantech/InterGPS" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/acl21_gps.txt" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://github.com/lupantech/InterGPS" target="_blank"><img src="https://img.shields.io/github/stars/lupantech/InterGPS?style=social" alt="GitHub stars"></a>
				<br>
				<font color="firebrick"><b>Oral Presentation</b></font>
				(*Equal Contribution)
			</div>
		</div>
		<hr>

		<!-- ACL21 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/acl21_soc.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues
					</font>
				</b><br>
				<a>Liang Qiu, Yuan Liang, Yizhou Zhao, <b>Pan Lu</b>, Baolin Peng, Zhou Yu, Ying Nian Wu, Song-Chun
					Zhu</a><br>
				<b><a href="https://2021.aclweb.org/" target="_blank">ACL 2021&nbsp;</a></b>
				<a href="https://arxiv.org/abs/2106.01006" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/acl21_soc.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="bibs/acl21_soc.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- AAAI20 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaai20_caption.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Learning Long- and Short-Term User Literal-Preference with Multimodal
						Hierarchical Transformer Network for Personalized Image Caption</font>
				</b><br>
				<a>Wei Zhang, Yue Ying, <strong>Pan Lu</strong>, Hongyuan Zha</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-20/" target="_blank">AAAI 2020&nbsp;</a></b>
				<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6503" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="papers/aaai20_caption.pdf" target="_blank"> <small>[PDF]&nbsp;</small></a>
				<a href="bibs/aaai20_caption.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- IJCAI19 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/ijcai19_matching.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Knowledge Aware Semantic Concept Expansion for Image-Text Matching</font>
				</b><br>
				<a>Botian Shi, Lei Ji, <strong>Pan Lu</strong>, Nan Duan</a><br>
				<b><a href="https://ijcai19.org/" target="_blank">IJCAI 2019&nbsp;</a></b>
				<a href="papers/ijcai19_matching.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/ijcai19_matching.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- CVPR19 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/cvpr19_dynamicvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Dynamic Fusion with Intra- and Inter-modality Attention Flow for Visual Question
						Answering</font>
				</b><br>
				<a>Peng Gao, Zhengkai Jiang, Haoxuan You, <strong>Pan Lu</strong>, Steven CH Hoi, Xiaogang Wang,
					Hongsheng Li</a><br>
				<b><a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019&nbsp;</a></b>
				<a href="papers/cvpr19_dynamicvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://github.com/bupt-cist/DFAF-for-VQA.pytorch" target="_blank">
					<small>[Code]&nbsp;</small></a>
				<a href="bibs/cvpr19_dynamicvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- PAKDD19 -->
		<!-- <div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/pakdd2019_hybrid.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b><font color="black">A Novel Hybrid Sequential Model for Review-based Rating Prediction</font></b><br>
				<a>Yuanquan Lu, Wei Zhang, <strong>Pan Lu</strong>, Jianyong Wang</a><br>
				<b><a href="https://pakdd2019.medmeeting.org/en" target="_blank">PAKDD 2019&nbsp;</a></b>
				<a href="papers/pakdd2019_hybrid.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/pakdd2019_hybrid.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div><hr> -->

		<!-- ICDE19  -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/icde19_knowledge.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Knowledge-Aware Deep Dual Networks for Text-Based Mortality Prediction</font>
				</b><br>
				<a>Ning Liu, <strong>Pan Lu</strong>, Wei Zhang, Jianyong Wang</a><br>
				<b><a href="http://conferences.cis.umac.mo/icde2019/" target="_blank">ICDE 2019&nbsp;</a></b>
				<a href="papers/icde19_knowledge.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/icde19_knowledge.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- ECCV18 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/eccv18_hybridvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Question-Guided Hybrid Convolution for Visual Question Answering</font>
				</b><br>
				<a>Peng Gao, Hongsheng Li, Shuang Li, <strong>Pan Lu</strong>, Yikang Li, Steven Hoi, Xiaogang
					Wang</a><br>
				<b><a href="https://eccv2018.org/" target="_blank">ECCV 2018&nbsp;</a></b>
				<a href="papers/eccv18_hybridvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="bibs/eccv18_hybridvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
			</div>
		</div>
		<hr>

		<!-- SIGKDD18 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/kdd18_rvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual
						Question Answering</font>
				</b><br>
				<a><strong>Pan Lu</strong>, Lei Ji, Wei Zhang, Nan Duan, Ming Zhou, Jianyong Wang</a><br>
				<b><a href="https://www.kdd.org/kdd2018/" target="_blank">SIGKDD 2018&nbsp;</a></b>
				<a href="papers/kdd18_rvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://github.com/lupantech/rvqa" target="_blank"> <small>[Project]&nbsp;</small></a>
				<a href="https://www.youtube.com/watch?v=OQoqpRuY4L4" target="_blank"> <small>[Video]&nbsp;</small></a>
				<a href="bibs/kdd18_rvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

		<!-- AAAI18 -->
		<div class="row">
			<div class="col-md-3">
				<img class="img-fluid img-rounded" src="imgs/aaa18_dualvqa.png" style="" alt="">
			</div>
			<div class="col-md-9">
				<b>
					<font color="black">Co-attending Free-form Regions and Detections with Multi-modal Multiplicative
						Feature Embedding for Visual Question Answering</font>
				</b><br>
				<a><strong>Pan Lu</strong>, Hongsheng Li, Wei Zhang, Jianyong Wang, Xiaogang Wang</a><br>
				<b><a href="https://aaai.org/Conferences/AAAI-18/" target="_blank">AAAI 2018&nbsp;</a></b>
				<a href="papers/aaa18_dualvqa.pdf" target="_blank"> <small>[Paper]&nbsp;</small></a>
				<a href="https://github.com/lupantech/dual-mfa-vqa" target="_blank"> <small>[Code]&nbsp;</small></a>
				<a href="bibs/aaa18_dualvqa.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
			</div>
		</div>
		<hr>

	</div><br>


	<!-- Experience -->
	<!-- <div class="container">
		<h3 id="Experience" style="">Selected Experience</h3>
		<hr>
		<ul>
			<li>
				<strong>Research Intern</strong>, <em>Microsoft Research</em>, 2022.12 - 2023.12 <br>
				Mentors: 
				Dr. <a href="https://sites.google.com/site/hcheng2site" target="_blank">Hao Cheng</a>,
				Dr. <a href="https://www.microsoft.com/en-us/research/people/mgalley/" target="_blank">Michel Galley</a>,
				Dr. <a href="https://www.microsoft.com/en-us/research/people/jfgao/" target="_blank">Jianfeng Gao</a>
			</li>
			<li>
				<strong>Research Intern</strong>, <em>Allen Institute for AI (AI2)</em>, 2022.3 - 2022.12 <br>
				Mentors: Dr. <a href="http://ashwinkalyan.com/" target="_blank">Ashwin Kalyan</a>,
				Dr. <a href="https://allenai.org/team/peterc" target="_blank">Peter Clark</a>
			</li>
			<li>
				<strong>Research Intern</strong>, <em>Document Intelligence Group</em>, <em>Adobe Research</em>, 2021.6
				- 2021.9 <br>
				Mentors: Dr. <a href="https://research.adobe.com/person/jiuxiang-gu/" target="_blank">Jiuxiang Gu</a>,
				Dr. <a href="https://research.adobe.com/person/tong-sun/" target="_blank">Tong Sun</a>
			</li>
			<li>
				<strong>Research Intern</strong>, <em>Natural Language Computing Group</em>, <em> Microsoft Research
					Asia</em>, 2017.11 - 2018.5<br>
					Mentors: Dr. <a href="https://www.microsoft.com/en-us/research/people/leiji/" target="_blank">Lei
					Ji</a>, Dr. <a href="https://www.microsoft.com/en-us/research/people/nanduan/" target="_blank">Nan
					Duan</a>, Dr. <a href="https://www.microsoft.com/en-us/research/people/mingzhou/"
					target="_blank">Ming Zhou</a>
			</li>
			<li>
				<strong>Research Assistant</strong>, <em>Multimedia Lab</em>, <em>Chinese University of Hong Kong</em>,
				2016.8 - 2017.10 <br>
				Advisors: Prof. <a href="https://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Hongsheng Li</a>, Prof. <a
					href="https://www.ee.cuhk.edu.hk/~xgwang/" target="_blank">Xiaogang Wang</a>
			</li>
		</ul>
	</div><br> -->


	<!-- Teaching -->
	<div class="container">
		<h3 id="Teaching" style="">Teaching</h3>
		<hr>
		<h4 id="Teaching" style="">Guest Lecturers</h4>
		<ul>
			<li>
				<strong>Guest Lecturer</strong>, <em>CSE 244C: Deep Learning for Advanced Computer Vision
				</em> <a href="https://courses.engineering.ucsc.edu/courses/cse244c" target="_blank">[Website]</a> <br>
				Graduate, UC Santa Cruz, Spring 2025
			</li>
			<li>
				<strong>Guest Lecturer</strong>, <em>LLMs and Mathematics Seminar</em> <a href="https://ayushkhaitanrutgers.github.io/seminar.html" target="_blank">[Website]</a> <br>
				Graduate, Rutgers University, Spring 2025
			</li>
			<li>
				<strong>Guest Lecturer</strong>, <em>CSCE 638: Natural Language Processing: Foundation and Techniques</em> <a href="https://khhuang.me/CSCE638-S25/" target="_blank">[Website]</a> <br>
				Graduate, TAMU, Spring 2025
			</li>
			<li>
				<strong>Guest Lecturer</strong>, <em>CS 263: Natural Language Processing</em> <a href="docs/UCLA CS263 NLP_Guest Lecture_Pan Lu_2023.05.31.pdf" target="_blank">[Slides]</a> <br>
				Graduate, UCLA, Spring 2023
			</li>
			<li>
				<strong>Guest Lecturer</strong>, <em>ECE C247: Neural Networks and Deep Learning</em>  <a href="" target="_blank">[Slides]</a> <br>
				Graduate, UCLA, Winter 2022
			</li>
			<li>
				<strong>Guest Lecturer</strong>, <em>Data Mining: Theory and Algorithms</em><br>
				Graduate, Tsinghua University, Fall 2017
			</li>

		</ul>
		<h4 id="Teaching" style="">Teaching Assistants</h4>
		<ul>
			<li>
				<strong>Teaching Associate</strong>, <em>ECE C247: Neural Networks and Deep Learning</em><br>
				Graduate, UCLA, Winter 2023
			</li>
			<li>
				<strong>Teaching Assistant</strong>, <em>CS 249: Data Science Fundamentals</em><br>
				Graduate, UCLA, Fall 2022
			</li>
			<li>
				<strong>Teaching Assistant</strong>, <em>ECE C247: Neural Networks and Deep Learning</em><br>
				Graduate, UCLA, Winter 2022
			</li>
			<li>
				<strong>Teaching Assistant</strong>, <em>CS 111: Operating System Principles</a></em><br>
				Undergraduate, UCLA, Fall 2021
			</li>
			<li>
				<strong>Teaching Assistant</strong>, <em>C Programming Language</a></em><br>
				Undergraduate, Tsinghua University, Spring 2016
			</li>
		</ul>
	</div><br>


	<!-- Service -->
	<div class="container">
		<h3 id="Service" style="">Professional Service</h3>
		<hr>
		<h4 id="Service" style="">Conferences</h4>
			<ul>
				<li>
					<strong>Senior Program Chair</strong>, 
					<a href="https://nenlp.github.io/spr2025/" target="_blank">New England NLP Meeting (NENLP) 2025</a>,
					New Haven, 2025.04
				</li>
				<li>
					<strong>Program Chair</strong>, 
					<a href="https://socalnlp.github.io/symp23/index.html" target="_blank">The 4th Southern California Natural Language Symposium (SoCal NLP)</a>,
					Los Angeles, 2023.11
				</li>
				<li>
					<strong>Senior Area Chair</strong>, 
					<a>EMNLP 2025</a>
				</li>
				<li>
					<strong>Area Chair</strong>, 
					<a>NeurIPS 2025</a>
				</li>
				<li>
					<strong>Area Chair</strong>, 
					<a>ICLR 2025</a>
				</li>
				<li>
					<strong>Area Chair</strong>, 
					<a>ACL 2025</a>
				</li>
			</ul>
		<h4 id="Service" style="">Workshops and Tutorials</h4>
			<ul>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://mathai2025.github.io/" target="_blank">MATH-AI: The 5th Workshop on Mathematical Reasoning and AI</a> 
					at NeurIPS 2025
				</li>
				<li>
					<strong>Co-organizer</strong>, Tutorial on
					<a href="https://cvpr2025mutimodalmathreasoning.github.io/MM_math_reasoning/" target="_blank">Multimodal Mathematical Reasoning</a> 
					at CVPR 2025
				</li>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://mathai2024.github.io/" target="_blank">MATH-AI: The 4th Workshop on Mathematical Reasoning and AI</a> 
					at NeurIPS 2024
				</li>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://sites.google.com/view/ai4mathworkshopicml2024" target="_blank">AI for Math</a> 
					at ICML 2024
				</li>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://sites.google.com/view/tavi-cvpr24/" target="_blank">Tool-Augmented VIsion (TAVI)</a> 
					at CVPR 2024
				</li>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://mathai2023.github.io/" target="_blank">MATH-AI: The 3rd Workshop on Mathematical Reasoning and AI</a> 
					at NeurIPS 2023
				</li>
				<li>
					<strong>Co-organizer</strong>, Tutorial on
					<a href="https://ijcai-23.org/" target="_blank">Deep Learning in Mathematical Reasoning: Recent Advances and Beyond</a> 
					at IJCAI 2023
				</li>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://mathai2022.github.io/" target="_blank">MATH-AI: Toward Human-Level Mathematical Reasoning</a> 
					at NeurIPS 2022
				</li>
				<li>
					<strong>Co-organizer</strong>, Workshop on
					<a href="https://mathai4ed.github.io/" target="_blank">Math AI for Education: Bridging
						the Gap Between Research and Smart Education</a> 
					at NeurIPS 2021
				</li>
			</ul>
			<h4 id="Service" style="">Program Committee Member</h4>
				<ul>
					<li> <strong>2024</strong>: NeurIPS, ICLR, ICML, CVPR, COLM, ARR, SIGKDD</li>
					<li> <strong>2023</strong>: NeurIPS, ICLR, ACL, EMNLP, AAAI</li>
					<li> <strong>2022</strong>: NeurIPS, ICLR, CVPR, AAAI,
						COLING</li>
					<li> <strong>2021</strong>: NeurIPS, ICLR, CVPR, ICCV</li>
					<li> <strong>2020 and before</strong>: NeurIPS, AAAI, SIGKDD, ICDM, PAKDD</li>
				</ul>
				<h4 id="Service" style="">Journal Reviewer</h3>
					<ul>
						<li>
							<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34"
								target="_blank">IEEE Transactions on Pattern Analysis and Machine Intelligence</a>
						</li>
						<li>
							<a href="https://www.springer.com/journal/11263/"
								target="_blank">International Journal of Computer Vision</a>
						</li>
						<li>
							<a href="https://www.sciencedirect.com/journal/expert-systems-with-applications"
								target="_blank">Expert Systems With Applications</a>
						</li>
						<li>
							<a href="http://www.ieee-jas.org/" target="_blank">IEEE/CAA JAS</a>,
							<a href="http://www.aas.net.cn/CN/volumn/current.shtml" target="_blank">AAS</a>
						</li>
					</ul>
				<h4 id="Service" style="">Organizations</h3>
					</ul>
						<li> <strong>Chair</strong>,
							IEEE Student Branch at Tsinghua University, Beijing, 2015.10 - 2016.10
						</li>
					</ul>
	</div><br>

	<!-- Awards -->
	<div class="container">
		<h3 id="Awards" style="">Selected Awards</h3>
		<hr>
		<ul>
			<li>
				🏆 <strong>World's Top 2% Scientists, Elsevier</strong>, 2025
				<a href="https://topscinet.com/scientist_profile/Lu,%20Pan/2017/?stype=single_year" target="_blank">[Link]</a>≈
			</li>
			<li>
				🏆 <strong>Most Influential ICLR Papers</strong> (ICLR 2024), 2025
				<a href="https://resources.paperdigest.org/2025/03/most-influential-iclr-papers-2025-03-version/" target="_blank">[List]</a>
			</li>
			<li>
				🏆 <strong>Best Paper Award, KnowledgeNLP Workshop-NAACL 2025</strong>, 
				2024
				<a href="https://knowledge-nlp.github.io/naacl2025/index.html" target="_blank">[Link]</a>
			</li>
			<li>
				🏆 <strong>Most Influential NIPS Papers</strong> (NeurIPS 2022), 2024
				<a href="https://www.paperdigest.org/2024/09/most-influential-nips-papers-2024-09/" target="_blank">[List]</a>
			</li>
			<li>
				🏆 <strong>Best Paper Honorable Mention Award, EMNLP 2024</strong>, 2024
			</li>
			<li>
				🧑‍🎓 <strong>Bloomberg Data Science Ph.D. Fellowship</strong>, 
				2023-2024
				<a href="https://www.bloomberg.com/company/stories/introducing-the-sixth-cohort-of-bloomberg-data-science-ph-d-fellows-2023-2024/" target="_blank">[Bloomberg]</a>
				<a href="https://samueli.ucla.edu/ucla-samueli-in-the-news" target="_blank">[UCLA Engineering]</a>
				<a href="https://www.cs.ucla.edu/cs-ph-d-pan-lu-awarded-bloomberg-data-science-ph-d-fellowship/" target="_blank">[UCLA CS]</a>
			</li>
			<li><strong>EMNLP 2024 Diversity and Inclusion Award</strong>, 2023</li>
			<li><strong>NeurIPS 2023 Scholar Award</strong>, 2023</li>
			<li>
				🧑‍🎓 <strong>Qualcomm Innovation Fellowship</strong>, 2023
				<a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2023-north-america" target="_blank">[Qualcomm]</a>
				<a href="https://www.cs.ucla.edu/phd-student-pan-lu-wins-2023-qualcomm-innovation-fellowship/" target="_blank">[UCLA CS]</a>
			</li>
			<li>
				🧑‍🎓 <strong>UCLA Dissertation Year Fellowship</strong>, 2023</li>
			<li>
				🧑‍🎓 <strong>Amazon PhD Fellowship</strong>, 2023
				<a href="https://www.sciencehub.ucla.edu/2023-amazon-fellows/" target="_blank">[UCLA/Amazon]</a>
			</li>
			<li><strong>NeurIPS 2022 Scholar Award</strong>, 2022</li>
			<!-- <li><strong>AAAI Travel Award</strong>, 2023</li> -->
			<li><strong>NeurIPS 2022 Top Reviewer</strong>, 2022</li>
			<!-- <li><strong>NeurIPS 2021 Scholar Award</strong>, 2021</li> -->
			<li><strong>ICLR 2022 Highlighted Reviewer</strong>, 2022</li>
			<li><strong>ACL-IJCNLP 2021 Diversity & Inclusion Award</strong>, 2021</li>
			<!-- <li><strong>NeurIPS Travel Award</strong>, 2019</li> -->
			<!-- <li><strong>ICCV Travel Award</strong>, 2019</li> -->
			<li><strong>Outstanding Master Thesis Award</strong>, Tsinghua University, 2018</li>
			<li><strong>Outstanding Graduate Award of Computer Science</strong>, Tsinghua University, 2018</li>
			<!-- <li><strong>SIGKDD Travel Award</strong>, 2018</li> -->
			<li><strong>"Stars of Tomorrow" Excellent Intern Award</strong>, Microsoft Research, 2018</li>
			<!-- <li><strong>AAAI Travel Award</strong>, 2018, 2020</li> -->
			<li><strong>GuangHua Scholarship Award</strong>, Tsinghua University, 2016</li>
			<li><strong>Outstanding Undergraduate, Beijing</strong>, 2015</li>
			<li><strong>Champion of National College Student Innovation Conference</strong>, 2014</li>
			<li><strong>National Scholarship</strong> (<em>for top 2% students</em>), 2014</li>
			<li><strong>First Prize of America Mathematical Contest In Modeling (MCM)</strong>, 2014</li>
			<li><strong>Xu Teli President Scholarship</strong> (<em>the highest honor for top 2
					undergraduates</em>), 2013</li>
		</ul>
	</div><br>

	<!-- Tweets -->
	<!-- https://publish.twitter.com/?buttonType=FollowButton&query=%40lupantech&widget=Timeline -->
	<!-- <div class="container">
		<h3 id="Tweets" style="">Tweets</h3>
		<hr>
		<ul>
			<div class="container" style="text-align: left;">
				<a class="twitter-timeline" data-width="50%" data-height="500" data-theme="light"
					href="https://twitter.com/lupantech?ref_src=twsrc%5Etfw">Tweets by lupantech</a>
				<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
			</div>
		</ul>
	</div><br> -->

	<!-- Contact -->
	<div class="container">
		<h3 id="Contact" style="">Contact</h3>
		<hr>
		350 Jane Stanford Way <br>
		Stanford, CA 94305 <br>
		lupantech [at] gmail [dot] com <br>
		<!-- <a href="mailto:lupantech@gmail.com">lupantech@gmail.com</a><br> -->
		<a href="https://scholar.google.com/citations?user=IyucsdQAAAAJ&hl=en" target="_blank">[<span
				style="color:#4285F4">G</span><span style="color:#DB4437">o</span><span
				style="color:#F4B400">o</span><span style="color:#4285F4">g</span><span
				style="color:#0F9D58">l</span><span style="color:#DB4437">e</span> Scholar]
		</a> &nbsp;| &nbsp;
		<a href="https://www.semanticscholar.org/author/Pan-Lu/2887562" target="_blank">[Semantic Scholar]</a> &nbsp;|&nbsp;
		<a href="https://github.com/lupantech" target="_blank">[GitHub]</a> &nbsp;|&nbsp;
		<a href="https://www.linkedin.com/in/pan-lu-9308909a/" target="_blank">[LinkedIn]</a>
	</div>


	<!-- Footer -->
	<div class="container">
		<hr>
		<!-- <center>
			<footer>
				<p>&copy; Pan Lu 2019</p>
			</footer>
		</center> -->
		<div class="row">
			<div class="col-md-9">
				<p align="left">&copy; Pan Lu 2025</p>
			</div>
			<div class="col-md-3">
				<script type="text/javascript" id="clustrmaps"
					src="//cdn.clustrmaps.com/map_v2.js?d=kM7hb210psDEOVXSuHck6GmcfISVxTpZ0I6EmgBHZMQ&cl=ffffff&w=a"></script>
			</div>
		</div>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
		integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
		crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"
		integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4"
		crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js"
		integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1"
		crossorigin="anonymous"></script>

	<script>
function showMore(){
  var dots = document.getElementById('dots');
  var moreContent = document.getElementById('more');
  var showMoreBtn = document.getElementById('showMoreBtn');

  if (dots.style.display === "none") {
    dots.style.display = "inline";
    showMoreBtn.innerHTML = "Show more";
    moreContent.style.display = "none";
  } else {
    dots.style.display = "none";
    showMoreBtn.innerHTML = "Show less";
    moreContent.style.display = "inline";
  }
}
	</script>

</body>

</html>